{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 93134\n",
      "    Root location: data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomHorizontalFlip(p=0.2)\n",
      "               RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data_dir = \"data/train\"\n",
    "train_dataset = datasets.ImageFolder(train_data_dir, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "val_data_dir = \"data/val\"\n",
    "val_dataset = datasets.ImageFolder(val_data_dir, transform=val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0458, -0.0458, -0.0458,  ...,  0.0227,  0.0227,  0.0227],\n",
      "         [-0.0458, -0.0458, -0.0458,  ...,  0.0398,  0.0398,  0.0398],\n",
      "         [-0.0458, -0.0458, -0.0458,  ...,  0.0398,  0.0398,  0.0398],\n",
      "         ...,\n",
      "         [-1.8782, -1.8782, -1.8782,  ..., -0.8164, -0.6281, -0.4397],\n",
      "         [-1.8782, -1.8782, -1.8782,  ..., -1.1247, -0.9363, -0.6965],\n",
      "         [-1.8782, -1.8782, -1.8782,  ..., -1.3815, -1.1932, -0.9534]],\n",
      "\n",
      "        [[ 0.1176,  0.1176,  0.1176,  ...,  0.1877,  0.1877,  0.1877],\n",
      "         [ 0.1176,  0.1176,  0.1176,  ...,  0.2052,  0.2052,  0.2052],\n",
      "         [ 0.1176,  0.1176,  0.1176,  ...,  0.2052,  0.2052,  0.2052],\n",
      "         ...,\n",
      "         [-1.7906, -1.7906, -1.7906,  ..., -0.6877, -0.4951, -0.3025],\n",
      "         [-1.7906, -1.7906, -1.7906,  ..., -1.0028, -0.8102, -0.5826],\n",
      "         [-1.7906, -1.7906, -1.7906,  ..., -1.2654, -1.0903, -0.8452]],\n",
      "\n",
      "        [[ 0.2871,  0.2871,  0.2871,  ...,  0.3568,  0.3568,  0.3742],\n",
      "         [ 0.2871,  0.2871,  0.2871,  ...,  0.3742,  0.3742,  0.3742],\n",
      "         [ 0.2871,  0.2871,  0.2871,  ...,  0.3742,  0.3742,  0.3742],\n",
      "         ...,\n",
      "         [-1.5953, -1.5953, -1.5953,  ..., -0.4275, -0.2358, -0.0441],\n",
      "         [-1.5953, -1.5953, -1.5953,  ..., -0.7587, -0.5495, -0.3230],\n",
      "         [-1.6127, -1.6127, -1.6127,  ..., -1.0201, -0.8284, -0.6018]]])\n"
     ]
    }
   ],
   "source": [
    "data = train_loader.dataset\n",
    "print(data[0][0])\n",
    "# use torchtoPIL to convert the tensor to a PIL Image\n",
    "img = transforms.ToPILImage()(data[445][0]) \n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  908585\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights= None)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', weights= None)\n",
    "# generate random seed\n",
    "seed = random.randint(0, 1000000)\n",
    "# set the seed\n",
    "print(\"Seed: \", seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class AlexNet(nn.Module): \n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = AlexNet()\n",
    "# open all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# change the last layer\n",
    "# model.fc = nn.Sequential(\n",
    "#     nn.Linear(512, 1000),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.2),\n",
    "#     nn.Linear(1000, 1000),\n",
    "#     nn.LogSoftmax(dim=1)\n",
    "# )\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\SWORWOOD/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# set the seed\n",
    "seed = random.randint(0, 1000000)\n",
    "print(\"Seed: \", seed)\n",
    "torch.manual_seed(seed)\n",
    "# Resnet\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights= None)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = torch.load(\"best_model_0.359.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:34<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 6.4892 train acc: 0.0110\n",
      "Epoch 1 val loss: 6.2070 val acc: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 5.7371 train acc: 0.0353\n",
      "Epoch 2 val loss: 5.7015 val acc: 0.0250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 5.1832 train acc: 0.0739\n",
      "Epoch 3 val loss: 5.3992 val acc: 0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 4.7166 train acc: 0.1180\n",
      "Epoch 4 val loss: 4.7428 val acc: 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 4.3053 train acc: 0.1679\n",
      "Epoch 5 val loss: 4.5886 val acc: 0.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 3.9513 train acc: 0.2161\n",
      "Epoch 6 val loss: 4.5349 val acc: 0.1355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 3.6455 train acc: 0.2616\n",
      "Epoch 7 val loss: 3.9603 val acc: 0.2075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 3.3872 train acc: 0.3027\n",
      "Epoch 8 val loss: 3.8984 val acc: 0.2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 3.1511 train acc: 0.3433\n",
      "Epoch 9 val loss: 3.6070 val acc: 0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss: 2.9460 train acc: 0.3788\n",
      "Epoch 10 val loss: 3.3347 val acc: 0.3040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train loss: 2.7719 train acc: 0.4103\n",
      "Epoch 11 val loss: 3.2401 val acc: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 train loss: 2.6048 train acc: 0.4390\n",
      "Epoch 12 val loss: 3.2351 val acc: 0.3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 train loss: 2.4643 train acc: 0.4657\n",
      "Epoch 13 val loss: 2.9963 val acc: 0.3715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 train loss: 2.3265 train acc: 0.4928\n",
      "Epoch 14 val loss: 2.8041 val acc: 0.3970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 train loss: 2.2004 train acc: 0.5162\n",
      "Epoch 15 val loss: 2.7687 val acc: 0.3955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 train loss: 2.0945 train acc: 0.5362\n",
      "Epoch 16 val loss: 2.5610 val acc: 0.4295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 train loss: 1.9774 train acc: 0.5589\n",
      "Epoch 17 val loss: 2.6357 val acc: 0.4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 train loss: 1.8842 train acc: 0.5774\n",
      "Epoch 18 val loss: 2.4175 val acc: 0.4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 train loss: 1.7918 train acc: 0.5970\n",
      "Epoch 19 val loss: 2.3900 val acc: 0.4775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 train loss: 1.7076 train acc: 0.6125\n",
      "Epoch 20 val loss: 2.3450 val acc: 0.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 train loss: 1.6246 train acc: 0.6282\n",
      "Epoch 21 val loss: 2.3303 val acc: 0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 train loss: 1.5502 train acc: 0.6455\n",
      "Epoch 22 val loss: 2.2784 val acc: 0.4945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 train loss: 1.4793 train acc: 0.6594\n",
      "Epoch 23 val loss: 2.1753 val acc: 0.5255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 train loss: 1.4120 train acc: 0.6732\n",
      "Epoch 24 val loss: 2.2353 val acc: 0.5020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 train loss: 1.3422 train acc: 0.6880\n",
      "Epoch 25 val loss: 2.2073 val acc: 0.5180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 train loss: 1.2811 train acc: 0.7008\n",
      "Epoch 26 val loss: 2.2828 val acc: 0.5030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 train loss: 1.2222 train acc: 0.7133\n",
      "Epoch 27 val loss: 2.0880 val acc: 0.5390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 train loss: 1.1610 train acc: 0.7285\n",
      "Epoch 28 val loss: 2.0077 val acc: 0.5475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 train loss: 1.1102 train acc: 0.7394\n",
      "Epoch 29 val loss: 2.2242 val acc: 0.5210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 train loss: 1.0625 train acc: 0.7492\n",
      "Epoch 30 val loss: 2.0509 val acc: 0.5510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 train loss: 1.0077 train acc: 0.7604\n",
      "Epoch 31 val loss: 1.9579 val acc: 0.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 train loss: 0.9588 train acc: 0.7715\n",
      "Epoch 32 val loss: 1.9888 val acc: 0.5655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 train loss: 0.9180 train acc: 0.7830\n",
      "Epoch 33 val loss: 2.0465 val acc: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 train loss: 0.8742 train acc: 0.7928\n",
      "Epoch 34 val loss: 2.0161 val acc: 0.5740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 train loss: 0.8323 train acc: 0.8009\n",
      "Epoch 35 val loss: 1.9457 val acc: 0.5825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 train loss: 0.7944 train acc: 0.8095\n",
      "Epoch 36 val loss: 1.9070 val acc: 0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 train loss: 0.7590 train acc: 0.8174\n",
      "Epoch 37 val loss: 1.8644 val acc: 0.5840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 train loss: 0.7205 train acc: 0.8276\n",
      "Epoch 38 val loss: 1.9790 val acc: 0.5745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 train loss: 0.6832 train acc: 0.8349\n",
      "Epoch 39 val loss: 1.9725 val acc: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 train loss: 0.6519 train acc: 0.8431\n",
      "Epoch 40 val loss: 1.8919 val acc: 0.5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 train loss: 0.6257 train acc: 0.8488\n",
      "Epoch 41 val loss: 1.8477 val acc: 0.6030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:32<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 train loss: 0.5914 train acc: 0.8571\n",
      "Epoch 42 val loss: 1.8709 val acc: 0.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 train loss: 0.5628 train acc: 0.8639\n",
      "Epoch 43 val loss: 2.1633 val acc: 0.5515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 train loss: 0.5366 train acc: 0.8706\n",
      "Epoch 44 val loss: 1.9395 val acc: 0.5805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 train loss: 0.5126 train acc: 0.8754\n",
      "Epoch 45 val loss: 1.9018 val acc: 0.5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 train loss: 0.4897 train acc: 0.8813\n",
      "Epoch 46 val loss: 1.8752 val acc: 0.5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [01:33<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 train loss: 0.4661 train acc: 0.8869\n",
      "Epoch 47 val loss: 1.8956 val acc: 0.5980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 305/1456 [00:19<01:13, 15.65it/s]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "# cuda settings\n",
    "torch.cuda.empty_cache()  # 清空GPU缓存\n",
    "torch.backends.cudnn.benchmark = True  # 启用自动寻找最适合当前配置的高效算法\n",
    "torch.backends.cudnn.enabled = True  # 启用cudnn加速\n",
    "# training\n",
    "batch_size = 500\n",
    "learning_rate = 0.00005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "epochs = 50\n",
    "best_acc = 0\n",
    "last_file = ''\n",
    "# train in cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in tqdm.tqdm(enumerate(train_loader, 0), total=len(train_loader)):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_acc += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "    running_loss /= len(train_loader)\n",
    "    running_acc /= len(train_dataset)\n",
    "    train_loss.append(running_loss)\n",
    "    train_acc.append(running_acc)\n",
    "    print(f\"Epoch {epoch+1} train loss: {running_loss:.4f} train acc: {running_acc:.4f}\")\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_running_acc += (predicted == labels).sum().item()\n",
    "    val_running_loss /= len(val_loader)\n",
    "    val_running_acc /= len(val_dataset)\n",
    "    val_loss.append(val_running_loss)\n",
    "    val_acc.append(val_running_acc)\n",
    "    print(f\"Epoch {epoch+1} val loss: {val_running_loss:.4f} val acc: {val_running_acc:.4f}\")\n",
    "    model.train()\n",
    "    # save best model\n",
    "    if val_running_acc > best_acc:\n",
    "        best_acc = val_running_acc\n",
    "        torch.save(model, 'best_model_'+str(best_acc)+'.pt')\n",
    "        if last_file != '':\n",
    "            os.remove(last_file)\n",
    "        last_file = 'best_model_'+str(best_acc)+'.pt'\n",
    "    torch.save(model, 'model_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataSet():\n",
    "    def __init__(self):\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.X_test = []\n",
    "        self.y_test = []\n",
    "        self.X_val = []\n",
    "        self.y_val = []\n",
    "        self.glo_mean = [] #for example [0.485, 0.456, 0.406] need to be calculated by the dataset\n",
    "        self.glo_std = []  #for example [0.229, 0.224, 0.225] need to be calculated by the dataset\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if not os.path.exists('DataSet'):\n",
    "            os.makedirs('DataSet')\n",
    "\n",
    "        if not os.path.exists('DataSet/X_train.pt'):\n",
    "            # generate basic train, test, val data and also the one hot label\n",
    "            self.load_data()\n",
    "            # generate mean and std\n",
    "            self.generate_mean_std()\n",
    "            # normalize the data from 0-255 to 0-1\n",
    "            self.normalize_data()\n",
    "            # zero expand the data from 3*64*64 to 3*224*224\n",
    "            self.zero_expand_data()\n",
    "            # save the data to DataSet folder\n",
    "            self.save_data()\n",
    "\n",
    "        self.X_train = torch.load('DataSet/X_train.pt').to(device)\n",
    "        self.y_train = torch.load('DataSet/y_train.pt').to(device)\n",
    "        self.X_test = torch.load('DataSet/X_test.pt').to(device)\n",
    "        self.y_test = torch.load('DataSet/y_test.pt').to(device)\n",
    "        self.X_val = torch.load('DataSet/X_val.pt').to(device)\n",
    "        self.y_val = torch.load('DataSet/y_val.pt').to(device)\n",
    "        self.glo_mean = torch.load('DataSet/glo_mean.pt').to(device)\n",
    "        self.glo_std = torch.load('DataSet/glo_std.pt').to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def prediction_accuracy(self, y_pred, y_true):\n",
    "        return sum(y_pred == y_true) / len(y_true)\n",
    "    \n",
    "    def one_hot_encode(self, label):\n",
    "        n = len(label)\n",
    "        one_hot_label = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            # a1_xxx -> xxx\n",
    "            hot = int(label[i].split('_')[1])\n",
    "            one_hot_label[i][hot] = 1\n",
    "        return one_hot_label\n",
    "    \n",
    "    def load_data(self):\n",
    "        current_dir = os.getcwd()\n",
    "        train_list = os.listdir(current_dir + '/data/train')\n",
    "        # Total classes\n",
    "        n = len(train_list)\n",
    "        # pic size 64*64*3\n",
    "        label = train_list\n",
    "        one_hot_label = self.one_hot_encode(label)\n",
    "        # sort by one_hot_label.argmax() zip with label\n",
    "        label = [x for _, x in sorted(zip(one_hot_label.argmax(axis=1), label))]\n",
    "        one_hot_label = self.one_hot_encode(label)\n",
    "\n",
    "        l = []\n",
    "        for i in label:\n",
    "            n = i.split('_')[1]\n",
    "            l.append(one_hot_label[int(n)][int(n)]==1)\n",
    "        # test if the one hot label is correct\n",
    "        assert all(l)\n",
    "\n",
    "        for i in train_list:\n",
    "            pic_list = os.listdir(current_dir + '/data/train/' + i)\n",
    "            for j in pic_list:\n",
    "                img = Image.open(current_dir + '/data/train/' + i + '/' + j)\n",
    "                img = np.array(img)\n",
    "                img = torch.tensor(img)\n",
    "                img = img.permute(2, 0, 1)\n",
    "                img = img.unsqueeze(0).float()\n",
    "                self.X_train.append(img)\n",
    "                self.y_train.append(one_hot_label[train_list.index(i)])\n",
    "        self.X_train = torch.cat(self.X_train)\n",
    "        self.y_train = torch.tensor(self.y_train)\n",
    "        self.y_train = self.y_train.argmax(1)\n",
    "        # test and val data\n",
    "\n",
    "        val_list = os.listdir(current_dir + '/data/val')\n",
    "        for i in val_list:\n",
    "            pic_list = os.listdir(current_dir + '/data/val/' + i)\n",
    "            for j in pic_list:\n",
    "                img = Image.open(current_dir + '/data/val/' + i + '/' + j)\n",
    "                img = np.array(img)\n",
    "                img = torch.tensor(img)\n",
    "                img = img.permute(2, 0, 1)\n",
    "                img = img.unsqueeze(0).float()\n",
    "                self.X_val.append(img)\n",
    "                self.y_val.append(one_hot_label[train_list.index(i)])\n",
    "        self.X_val = torch.cat(self.X_val)\n",
    "        self.y_val = torch.tensor(self.y_val)   \n",
    "        self.y_val = self.y_val.argmax(1)\n",
    "\n",
    "        test_list = os.listdir(current_dir + '/data/test')\n",
    "        for i in test_list:\n",
    "            pic_list = os.listdir(current_dir + '/data/test/' + i)\n",
    "            for j in pic_list:\n",
    "                img = Image.open(current_dir + '/data/test/' + i + '/' + j)\n",
    "                img = np.array(img)\n",
    "                img = torch.tensor(img)\n",
    "                img = img.permute(2, 0, 1)\n",
    "                img = img.unsqueeze(0).float()\n",
    "                self.X_test.append(img)\n",
    "                self.y_test.append(one_hot_label[train_list.index(i)])\n",
    "        self.X_test = torch.cat(self.X_test)\n",
    "        self.y_test = torch.tensor(self.y_test)\n",
    "        self.y_test = self.y_test.argmax(1)\n",
    "    \n",
    "    def generate_mean_std(self):\n",
    "        self.glo_mean = torch.mean(self.X_train, dim=(0, 2, 3))\n",
    "        self.glo_std = torch.std(self.X_train, dim=(0, 2, 3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWORWOOD\\AppData\\Local\\Temp\\ipykernel_6256\\229839884.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  self.y_train = torch.tensor(self.y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   0,  ..., 999, 999, 999])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dataSet' object has no attribute 'preprocess_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m DS \u001b[38;5;241m=\u001b[39m \u001b[43mdataSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m DS\u001b[38;5;241m.\u001b[39mload_data()\n",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mdataSet.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# generate basic train, test, val data and also the one hot label\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_data\u001b[49m()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# generate mean and std\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_mean_std()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dataSet' object has no attribute 'preprocess_data'"
     ]
    }
   ],
   "source": [
    "DS = dataSet()\n",
    "DS.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3, 64, 64])\n",
      "[0, 0, 0, 265541.0, 265414.0, 233293.0, 62203.0, 63304.0, 79953.0, 242942.0, 243833.0, 125092.0, 209958.0, 207115.0, 92151.0, 139143.0, 126088.0, 77472.0, 88548.0, 87541.0, 97774.0, 305799.0, 313239.0, 319700.0, 194565.0, 176934.0, 119390.0, 347494.0, 362106.0, 425449.0, 210262.0, 205815.0, 230781.0, 178081.0, 190511.0, 134656.0, 179588.0, 164064.0, 255590.0, 296598.0, 261629.0, 194381.0, 169033.0, 174913.0, 145491.0, 281563.0, 280007.0, 268081.0, 136796.0, 115263.0, 104578.0, 204120.0, 197565.0, 148184.0, 113362.0, 115134.0, 142051.0, 115976.0, 107927.0, 173868.0, 234462.0, 204142.0, 153202.0, 194269.0, 189529.0, 145925.0, 160618.0, 138900.0, 93577.0, 189053.0, 179480.0, 153141.0, 143517.0, 126748.0, 96751.0, 268327.0, 253309.0, 162615.0, 497403.0, 515026.0, 510414.0, 458310.0, 475989.0, 514394.0, 240090.0, 238276.0, 236160.0, 160529.0, 148711.0, 108252.0, 69914.0, 65450.0, 63266.0, 254485.0, 215068.0, 179875.0, 428192.0, 425366.0, 248010.0, 116918.0, 110408.0, 90729.0, 297836.0, 295854.0, 195089.0, 233854.0, 220099.0, 188720.0, 197414.0, 171655.0, 144740.0, 381049.0, 390944.0, 369702.0, 345662.0, 376127.0, 310576.0, 228443.0, 225589.0, 196041.0, 184458.0, 173037.0, 111395.0, 221970.0, 214801.0, 154151.0, 234133.0, 225785.0, 158818.0, 150962.0, 145051.0, 127587.0, 187930.0, 166710.0, 142984.0, 326873.0, 362333.0, 280796.0, 239664.0, 244448.0, 162082.0, 178225.0, 157655.0, 147775.0, 302145.0, 318177.0, 346694.0, 238315.0, 237954.0, 221729.0, 285731.0, 288490.0, 295614.0, 230835.0, 232948.0, 241323.0, 211934.0, 219702.0, 193372.0, 271623.0, 275014.0, 247563.0, 244330.0, 253752.0, 173968.0, 129527.0, 124195.0, 108977.0, 197988.0, 196356.0, 192272.0, 437381.0, 454519.0, 373104.0, 167661.0, 127169.0, 72919.0, 74785.0, 76281.0, 56948.0, 180032.0, 188574.0, 205018.0, 304869.0, 306782.0, 211771.0, 177001.0, 175438.0, 205035.0, 201681.0, 195308.0, 154295.0, 130008.0, 134553.0, 118465.0, 213748.0, 226019.0, 267456.0, 184164.0, 167484.0, 133557.0, 201184.0, 198991.0, 203933.0, 220306.0, 225201.0, 251981.0, 157260.0, 150300.0, 130245.0, 181079.0, 174025.0, 163957.0, 239197.0, 227099.0, 230974.0, 431438.0, 480674.0, 420737.0, 230394.0, 231400.0, 270701.0, 464805.0, 480167.0, 538216.0, 332607.0, 311596.0, 219233.0, 208310.0, 203528.0, 202068.0, 250285.0, 260811.0, 253544.0, 219568.0, 215152.0, 273953.0, 302430.0, 317226.0, 312562.0, 146677.0, 143980.0, 156556.0, 213509.0, 207344.0, 186411.0, 392393.0, 374278.0, 219174.0, 305615.0, 301851.0, 242497.0, 211558.0, 217180.0, 215995.0, 40189.0, 34454.0, 17833.0, 66563.0, 58783.0, 45090.0, 336857.0, 338586.0, 308479.0, 127058.0, 121636.0, 102013.0, 150565.0, 124551.0, 56684.0, 437734.0, 448529.0, 454600.0, 136148.0, 135061.0, 130499.0, 472187.0, 490071.0, 459724.0, 381489.0, 396450.0, 425698.0, 166370.0, 171720.0, 190268.0, 375947.0, 396498.0, 275626.0, 324053.0, 310529.0, 289065.0, 413539.0, 388332.0, 481077.0, 296803.0, 298175.0, 308387.0, 220764.0, 212872.0, 146272.0, 81676.0, 73589.0, 71885.0, 372519.0, 377873.0, 355251.0, 69835.0, 61471.0, 37214.0, 110013.0, 108165.0, 117040.0, 263110.0, 264969.0, 339089.0, 125133.0, 122757.0, 119058.0, 161569.0, 147360.0, 103071.0, 425994.0, 433886.0, 413485.0, 263381.0, 241219.0, 160819.0, 292909.0, 248167.0, 205333.0, 145858.0, 139546.0, 105026.0, 286576.0, 289623.0, 302048.0, 132472.0, 136483.0, 128227.0, 160975.0, 150476.0, 94814.0, 85258.0, 84216.0, 34915.0, 251807.0, 240747.0, 289820.0, 130420.0, 121889.0, 133434.0, 256444.0, 242584.0, 312107.0, 313182.0, 319259.0, 432869.0, 353192.0, 363728.0, 381097.0, 211919.0, 199592.0, 169005.0, 349958.0, 354808.0, 344847.0, 228921.0, 246180.0, 230667.0, 310589.0, 325603.0, 293432.0, 391162.0, 411400.0, 379189.0, 237210.0, 233142.0, 282545.0, 97627.0, 82205.0, 43656.0, 160389.0, 159582.0, 179684.0, 236710.0, 238262.0, 240618.0, 164617.0, 148425.0, 90548.0, 217091.0, 199444.0, 178534.0, 81779.0, 78785.0, 69363.0, 45392.0, 40056.0, 30744.0, 342059.0, 339698.0, 325524.0, 252127.0, 253120.0, 229261.0, 139166.0, 138121.0, 126798.0, 91466.0, 93664.0, 79016.0, 144937.0, 153129.0, 174391.0, 90413.0, 86639.0, 66019.0, 374228.0, 382822.0, 341996.0, 305217.0, 314943.0, 372037.0, 288372.0, 268755.0, 196245.0, 188158.0, 192640.0, 168529.0, 233108.0, 208675.0, 167298.0, 160584.0, 161802.0, 110735.0, 305736.0, 288456.0, 247194.0, 399862.0, 410343.0, 371895.0, 251541.0, 254480.0, 217449.0, 326190.0, 325084.0, 194545.0, 120860.0, 109630.0, 87191.0, 189063.0, 161026.0, 113533.0, 183144.0, 191152.0, 119307.0, 187480.0, 185415.0, 249273.0, 235898.0, 219092.0, 157909.0, 191032.0, 179057.0, 136893.0, 162119.0, 159423.0, 153067.0, 55974.0, 50861.0, 109808.0, 182646.0, 173208.0, 136039.0, 77713.0, 69629.0, 54177.0, 188099.0, 195724.0, 179931.0, 43142.0, 35357.0, 23248.0, 454205.0, 531738.0, 202429.0, 168449.0, 173210.0, 152148.0, 140626.0, 137142.0, 99021.0, 290663.0, 279263.0, 291282.0, 155166.0, 157070.0, 141154.0, 296800.0, 292083.0, 274400.0, 202405.0, 195473.0, 213613.0, 82312.0, 71469.0, 72149.0, 126623.0, 123375.0, 157869.0, 243591.0, 248622.0, 263867.0, 171017.0, 169181.0, 155411.0, 209771.0, 211027.0, 191600.0, 97129.0, 91769.0, 97765.0, 315829.0, 325281.0, 391443.0, 330561.0, 341911.0, 452518.0, 279372.0, 270457.0, 245904.0, 239360.0, 242079.0, 244690.0, 150852.0, 134124.0, 89017.0, 290453.0, 292689.0, 295553.0, 305233.0, 315932.0, 316695.0, 341291.0, 348774.0, 216198.0, 240721.0, 238106.0, 254917.0, 230480.0, 221563.0, 198730.0, 267136.0, 230325.0, 163907.0, 340297.0, 350403.0, 390827.0, 281546.0, 286047.0, 222765.0, 154864.0, 152830.0, 137986.0, 253692.0, 268838.0, 270059.0, 62498.0, 56068.0, 34817.0, 144008.0, 144311.0, 114418.0, 579927.0, 598988.0, 575159.0, 144249.0, 128918.0, 101538.0, 274778.0, 281174.0, 167488.0, 324497.0, 327846.0, 313746.0, 42957.0, 37315.0, 17719.0, 127351.0, 132764.0, 144692.0, 340084.0, 339542.0, 338203.0, 246661.0, 238637.0, 253790.0, 277406.0, 284213.0, 409172.0, 186602.0, 198630.0, 241423.0, 198966.0, 211006.0, 163002.0, 200867.0, 198105.0, 201470.0, 135111.0, 136839.0, 120878.0, 248307.0, 248772.0, 213006.0, 257668.0, 257940.0, 260213.0, 216583.0, 213567.0, 192524.0, 106515.0, 103976.0, 119012.0, 256195.0, 260149.0, 210644.0, 392161.0, 409953.0, 507176.0, 306373.0, 287301.0, 161094.0, 322554.0, 275888.0, 123974.0, 203803.0, 202375.0, 201118.0, 246224.0, 242557.0, 208172.0, 281084.0, 276397.0, 346814.0, 244039.0, 241106.0, 254797.0, 150647.0, 138150.0, 100565.0, 483438.0, 503561.0, 517976.0, 139909.0, 137666.0, 139419.0, 291609.0, 288663.0, 242856.0, 315949.0, 296640.0, 278120.0, 408146.0, 422711.0, 394831.0, 199818.0, 196841.0, 155572.0, 469572.0, 489900.0, 469754.0, 217910.0, 216712.0, 119969.0, 154019.0, 155606.0, 169387.0, 371899.0, 370511.0, 422502.0, 454369.0, 445780.0, 402630.0, 518748.0, 510636.0, 483972.0, 291193.0, 267014.0, 303820.0, 278811.0, 296905.0, 287464.0, 101602.0, 84960.0, 51772.0, 244081.0, 235118.0, 167609.0, 208347.0, 217154.0, 198369.0, 57030.0, 46866.0, 71173.0, 385325.0, 386880.0, 411236.0, 254736.0, 261132.0, 273884.0, 120201.0, 113812.0, 99242.0, 204777.0, 176667.0, 93328.0, 195464.0, 189741.0, 165160.0, 228013.0, 232963.0, 224357.0, 69443.0, 67012.0, 50749.0, 196876.0, 206880.0, 223869.0, 309115.0, 258355.0, 116167.0, 133332.0, 134653.0, 127884.0, 202606.0, 184890.0, 404949.0, 405730.0, 416446.0, 353893.0, 210292.0, 215111.0, 244070.0, 309374.0, 316940.0, 370154.0, 162904.0, 163085.0, 134877.0, 232043.0, 222663.0, 185174.0, 251330.0, 249226.0, 238460.0, 425688.0, 430858.0, 479053.0, 396667.0, 396365.0, 369035.0, 170329.0, 170327.0, 98462.0, 303149.0, 307722.0, 311165.0, 195596.0, 185740.0, 89391.0, 296099.0, 296298.0, 272476.0, 273409.0, 273535.0, 276378.0, 79217.0, 71061.0, 44902.0, 249210.0, 248740.0, 345026.0, 223962.0, 222223.0, 247159.0, 288960.0, 299762.0, 254792.0, 176903.0, 165135.0, 124950.0, 218336.0, 207873.0, 153319.0, 136978.0, 114143.0, 75053.0, 162961.0, 148051.0, 126943.0, 262471.0, 253681.0, 211770.0, 222679.0, 224671.0, 198620.0, 174758.0, 173934.0, 165697.0, 232246.0, 206753.0, 152798.0, 237145.0, 228490.0, 305722.0, 234003.0, 207934.0, 97803.0, 193406.0, 183233.0, 178859.0, 197654.0, 214734.0, 257532.0, 116782.0, 115733.0, 90408.0, 167428.0, 180141.0, 148469.0, 375810.0, 383571.0, 387525.0, 220744.0, 230532.0, 254567.0, 234546.0, 231859.0, 219341.0, 214922.0, 200240.0, 153740.0, 160155.0, 147138.0, 112599.0, 211251.0, 222718.0, 215138.0, 208263.0, 220403.0, 253364.0, 192699.0, 195617.0, 241138.0, 317581.0, 327362.0, 259108.0, 206588.0, 205431.0, 241745.0, 227227.0, 226105.0, 214446.0, 399124.0, 329723.0, 518173.0, 153667.0, 113420.0, 56280.0, 158664.0, 162358.0, 148223.0, 123526.0, 103493.0, 193385.0, 263517.0, 251772.0, 228637.0, 427150.0, 431375.0, 266729.0, 39780.0, 37990.0, 22671.0, 142459.0, 132825.0, 100868.0, 119558.0, 119483.0, 109323.0, 242664.0, 245117.0, 263423.0, 334784.0, 342114.0, 345076.0, 260171.0, 240263.0, 138001.0, 230963.0, 229492.0, 207682.0, 274833.0, 267192.0, 263334.0, 274001.0, 282327.0, 312412.0, 190536.0, 174481.0, 115510.0, 104269.0, 94223.0, 70541.0, 327751.0, 343680.0, 415254.0, 154907.0, 156609.0, 208797.0, 235908.0, 224164.0, 141682.0, 132736.0, 136478.0, 185555.0, 206897.0, 186331.0, 146293.0, 362901.0, 368312.0, 350950.0, 254915.0, 245178.0, 177075.0, 256658.0, 259898.0, 232724.0, 299771.0, 298383.0, 320497.0, 194678.0, 201103.0, 186030.0, 126676.0, 123637.0, 86053.0, 561183.0, 580455.0, 598284.0, 384840.0, 392717.0, 372227.0, 119179.0, 106690.0, 66238.0, 310650.0, 297065.0, 256455.0, 103656.0, 93920.0, 56261.0, 222577.0, 219131.0, 190998.0, 216095.0, 190896.0, 94024.0, 260015.0, 259337.0, 214184.0, 489652.0, 501719.0, 499188.0, 191291.0, 187193.0, 139605.0, 229343.0, 241436.0, 206253.0, 234195.0, 194876.0, 116894.0, 207562.0, 199097.0, 228358.0, 163401.0, 147077.0, 104169.0, 420741.0, 427429.0, 383894.0, 238390.0, 252389.0, 282965.0, 215828.0, 233870.0, 398889.0, 164707.0, 157070.0, 111535.0, 382874.0, 399813.0, 470756.0, 199154.0, 197213.0, 206522.0, 267882.0, 277362.0, 278757.0, 182584.0, 176680.0, 187616.0, 58676.0, 51234.0, 27869.0, 445890.0, 447824.0, 449731.0, 143434.0, 150651.0, 133658.0, 152219.0, 144478.0, 118697.0, 143238.0, 148610.0, 112531.0, 188231.0, 173560.0, 162481.0, 132714.0, 113027.0, 86367.0, 373633.0, 369034.0, 319623.0, 214581.0, 203357.0, 153745.0, 206824.0, 195160.0, 168006.0, 71907.0, 67985.0, 23148.0, 221440.0, 222494.0, 222549.0, 412847.0, 421463.0, 394871.0, 412078.0, 384187.0, 215230.0, 254785.0, 265766.0, 204158.0, 385803.0, 391482.0, 301148.0, 146976.0, 125050.0, 79571.0, 314631.0, 311884.0, 289284.0, 532911.0, 551611.0, 495444.0, 317842.0, 306077.0, 268119.0, 119698.0, 112638.0, 96939.0, 190691.0, 195824.0, 192131.0, 344585.0, 362094.0, 359953.0, 578595.0, 568870.0, 551688.0, 155733.0, 156196.0, 123283.0, 432663.0, 437065.0, 438909.0, 261353.0, 257158.0, 340630.0, 206119.0, 174698.0, 120976.0, 206949.0, 193348.0, 124279.0, 129051.0, 124757.0, 91441.0, 222935.0, 219597.0, 170255.0, 146812.0, 148416.0, 132272.0, 242996.0, 248444.0, 225393.0, 180910.0, 179114.0, 169090.0, 261680.0, 242565.0, 161235.0, 114153.0, 109738.0, 107664.0, 166517.0, 168944.0, 172437.0, 353433.0, 362001.0, 354678.0, 111517.0, 102872.0, 92007.0, 255558.0, 241293.0, 110357.0, 140785.0, 130581.0, 122832.0, 136356.0, 130171.0, 142285.0, 105250.0, 108142.0, 99506.0, 160328.0, 143535.0, 85393.0, 258142.0, 266390.0, 315078.0, 390230.0, 394840.0, 397759.0, 172556.0, 166615.0, 147690.0, 257043.0, 256590.0, 266923.0, 121567.0, 115812.0, 90776.0, 299916.0, 331056.0, 333331.0, 127034.0, 119224.0, 96775.0, 94064.0, 94615.0, 97378.0, 74611.0, 68433.0, 129215.0, 131677.0, 132654.0, 131927.0, 304878.0, 313986.0, 293730.0, 288540.0, 297438.0, 175921.0, 375176.0, 389384.0, 420247.0, 159780.0, 164683.0, 159801.0, 234891.0, 236622.0, 240993.0, 123749.0, 121604.0, 101523.0, 294337.0, 216153.0, 326886.0, 334293.0, 343100.0, 345974.0, 181348.0, 153343.0, 142707.0, 255537.0, 254885.0, 138974.0, 261261.0, 256044.0, 226170.0, 454896.0, 470595.0, 491095.0, 358325.0, 372992.0, 441484.0, 92628.0, 92374.0, 68160.0, 200998.0, 192787.0, 192997.0, 85217.0, 71168.0, 38468.0, 340459.0, 346755.0, 281128.0, 602099.0, 612696.0, 589868.0, 201900.0, 196181.0, 150819.0, 127000.0, 118840.0, 69536.0, 507044.0, 509371.0, 469998.0, 150881.0, 147355.0, 119558.0, 258191.0, 256783.0, 213241.0, 417006.0, 430576.0, 472090.0, 277879.0, 291117.0, 213246.0, 204785.0, 203670.0, 192590.0, 333704.0, 365546.0, 358431.0, 84380.0, 77186.0, 57567.0, 241343.0, 238183.0, 166103.0, 100897.0, 97880.0, 72995.0, 233342.0, 233031.0, 197114.0, 53382.0, 53291.0, 25281.0, 74603.0, 79801.0, 48027.0, 414752.0, 419494.0, 200885.0, 165917.0, 176163.0, 163924.0, 273405.0, 298262.0, 152217.0, 511765.0, 515655.0, 477026.0, 202247.0, 225782.0, 147756.0, 62546.0, 64069.0, 61205.0, 357207.0, 368225.0, 357575.0, 328922.0, 334268.0, 378093.0, 422867.0, 411926.0, 364441.0, 273378.0, 270195.0, 170870.0, 196965.0, 194557.0, 182202.0, 58804.0, 49475.0, 26745.0, 318844.0, 317218.0, 286669.0, 77363.0, 72237.0, 34655.0, 280316.0, 234018.0, 127044.0, 146618.0, 140422.0, 93877.0, 123825.0, 112745.0, 191247.0, 268854.0, 257796.0, 203451.0, 101633.0, 104197.0, 90061.0, 167199.0, 141910.0, 79140.0, 311899.0, 322378.0, 363751.0, 233708.0, 262140.0, 250089.0, 427675.0, 448022.0, 431260.0, 286919.0, 290720.0, 206145.0, 356188.0, 352100.0, 310981.0, 202534.0, 190890.0, 151351.0, 353318.0, 347951.0, 293422.0, 188180.0, 191899.0, 168298.0, 372722.0, 364042.0, 370315.0, 136409.0, 137939.0, 122949.0, 129766.0, 118259.0, 91419.0, 166606.0, 152770.0, 101001.0, 233452.0, 238263.0, 334974.0, 347541.0, 359446.0, 356013.0, 364362.0, 353430.0, 382521.0, 316001.0, 306455.0, 341025.0, 265274.0, 235880.0, 159182.0, 200144.0, 197988.0, 159264.0, 278983.0, 268715.0, 211569.0, 280996.0, 284891.0, 279855.0, 141900.0, 153474.0, 194619.0, 243683.0, 262831.0, 248654.0, 355739.0, 369523.0, 307473.0, 154043.0, 152861.0, 127077.0, 61970.0, 56076.0, 26274.0, 277169.0, 228069.0, 152817.0, 186550.0, 166846.0, 131030.0, 174336.0, 168436.0, 88354.0, 260542.0, 262307.0, 207723.0, 414095.0, 414101.0, 412348.0, 405820.0, 422788.0, 433886.0, 331584.0, 335727.0, 329765.0, 454907.0, 443504.0, 361133.0, 153002.0, 196780.0, 69254.0, 74345.0, 71253.0, 60814.0, 127986.0, 109307.0, 71639.0, 134988.0, 108262.0, 121451.0, 80477.0, 70308.0, 58660.0, 149655.0, 142147.0, 125242.0, 128361.0, 121314.0, 93518.0, 169900.0, 168386.0, 151359.0, 91801.0, 90463.0, 70455.0, 349071.0, 345452.0, 248049.0, 306682.0, 314334.0, 329109.0, 274987.0, 273588.0, 225290.0, 204669.0, 205342.0, 158476.0, 196993.0, 199151.0, 298916.0, 318947.0, 308821.0, 432402.0, 390549.0, 395525.0, 377595.0, 284822.0, 281754.0, 261379.0, 104397.0, 103373.0, 60718.0, 164378.0, 152486.0, 130777.0, 138448.0, 139807.0, 149901.0, 126215.0, 124839.0, 70060.0, 122947.0, 118133.0, 103598.0, 391061.0, 385344.0, 247955.0, 172785.0, 171198.0, 202421.0, 247211.0, 254016.0, 352264.0, 461508.0, 467896.0, 483612.0, 427608.0, 439824.0, 442413.0, 468821.0, 482979.0, 518343.0, 206853.0, 229983.0, 301870.0, 275930.0, 290619.0, 325474.0, 175892.0, 152884.0, 84530.0, 220576.0, 233245.0, 246181.0, 142306.0, 147988.0, 143133.0, 120634.0, 112402.0, 62833.0, 166048.0, 132708.0, 89824.0, 351662.0, 354710.0, 318801.0, 250081.0, 249148.0, 226859.0, 136362.0, 130769.0, 101131.0, 227572.0, 228665.0, 238708.0, 372516.0, 346495.0, 240543.0, 124239.0, 108518.0, 54333.0, 165817.0, 161741.0, 180312.0, 398101.0, 356917.0, 154834.0, 166987.0, 162692.0, 136313.0, 252197.0, 248492.0, 433523.0, 159405.0, 156016.0, 123083.0, 266699.0, 283896.0, 235845.0, 176681.0, 168364.0, 135465.0, 235549.0, 238738.0, 229209.0, 561306.0, 576703.0, 534953.0, 343122.0, 351706.0, 219925.0, 127491.0, 115627.0, 96763.0, 158202.0, 148647.0, 122771.0, 335187.0, 331094.0, 333002.0, 293721.0, 320426.0, 309625.0, 343488.0, 359018.0, 309774.0, 139300.0, 132122.0, 98613.0, 136758.0, 124461.0, 100421.0, 152233.0, 148940.0, 140116.0, 151302.0, 142202.0, 93777.0, 275776.0, 271114.0, 204325.0, 294882.0, 301750.0, 277748.0, 416466.0, 432232.0, 405742.0, 106591.0, 100166.0, 98970.0, 241021.0, 237388.0, 181507.0, 285682.0, 284367.0, 258371.0, 216236.0, 205383.0, 221459.0, 71113.0, 69457.0, 55357.0, 163553.0, 137276.0, 97750.0, 214374.0, 214477.0, 300921.0, 93528.0, 83251.0, 55624.0, 169890.0, 157901.0, 174443.0, 109074.0, 99022.0, 77973.0, 199749.0, 211041.0, 205928.0, 162463.0, 130941.0, 63689.0, 154548.0, 151837.0, 154506.0, 275001.0, 263571.0, 247220.0, 154406.0, 147107.0, 112469.0, 236435.0, 234711.0, 203907.0, 415903.0, 418809.0, 363707.0, 238507.0, 200671.0, 169871.0, 118213.0, 115204.0, 102168.0, 204844.0, 198101.0, 197999.0, 137795.0, 136112.0, 93754.0, 163711.0, 151284.0, 97523.0, 64795.0, 74239.0, 51920.0, 173246.0, 172565.0, 156411.0, 379644.0, 400511.0, 436529.0, 382805.0, 392023.0, 406029.0, 306634.0, 297390.0, 249039.0, 109970.0, 102420.0, 103453.0, 265742.0, 242445.0, 204085.0, 156532.0, 141005.0, 54750.0, 137249.0, 132167.0, 118106.0, 87693.0, 83685.0, 70088.0, 165805.0, 166333.0, 140538.0, 242414.0, 248609.0, 275284.0, 180385.0, 173089.0, 126890.0, 359222.0, 355480.0, 447087.0, 194011.0, 180426.0, 122885.0, 199744.0, 184418.0, 112151.0, 273671.0, 273357.0, 260412.0, 238093.0, 240602.0, 213363.0, 259724.0, 296232.0, 282381.0, 134342.0, 135647.0, 133937.0, 117923.0, 114598.0, 94671.0, 403700.0, 409349.0, 380604.0, 222853.0, 203666.0, 148161.0, 72210.0, 74487.0, 70021.0, 421713.0, 429691.0, 154566.0, 172550.0, 161641.0, 109023.0, 249150.0, 277055.0, 307818.0, 231429.0, 235270.0, 216319.0, 182529.0, 188012.0, 172714.0, 246516.0, 214571.0, 166443.0, 177148.0, 181061.0, 161336.0, 148116.0, 141521.0, 101074.0, 134091.0, 131050.0, 179102.0, 220418.0, 229651.0, 134866.0, 35029.0, 29867.0, 17218.0, 263448.0, 244016.0, 167095.0, 135669.0, 121081.0, 90585.0, 202722.0, 193174.0, 156681.0, 241064.0, 175319.0, 160541.0, 158990.0, 170368.0, 118245.0, 333012.0, 331140.0, 227002.0, 291876.0, 281213.0, 205369.0, 108632.0, 110662.0, 124273.0, 129265.0, 122550.0, 83083.0, 499889.0, 524412.0, 476792.0, 548501.0, 558861.0, 542241.0, 201018.0, 198716.0, 138769.0, 275966.0, 275609.0, 230200.0, 176553.0, 153447.0, 116065.0, 67599.0, 52799.0, 28188.0, 76661.0, 77034.0, 77869.0, 222134.0, 219198.0, 204747.0, 267867.0, 253444.0, 178302.0, 422038.0, 445138.0, 376531.0, 348817.0, 349658.0, 342049.0, 365486.0, 369547.0, 364640.0, 378909.0, 396286.0, 283888.0, 169950.0, 179475.0, 177513.0, 319001.0, 262876.0, 381918.0, 357235.0, 373881.0, 335532.0, 585814.0, 577378.0, 670822.0, 170649.0, 153807.0, 92553.0, 197360.0, 189592.0, 170148.0, 115282.0, 111290.0, 91612.0, 151197.0, 149665.0, 158729.0, 128913.0, 127023.0, 94743.0, 193000.0, 190014.0, 183503.0, 183052.0, 186082.0, 140446.0, 292313.0, 267288.0, 178975.0, 244399.0, 243282.0, 210023.0, 199334.0, 201477.0, 237580.0, 173973.0, 167286.0, 178757.0, 138124.0, 119875.0, 82606.0, 107315.0, 100113.0, 74940.0, 212926.0, 198177.0, 163578.0, 129590.0, 115321.0, 81383.0, 168188.0, 162242.0, 96255.0, 221106.0, 200433.0, 106284.0, 386591.0, 394733.0, 385801.0, 500223.0, 505763.0, 472338.0, 112669.0, 108412.0, 95079.0, 201126.0, 195617.0, 204142.0, 153586.0, 155684.0, 153473.0, 226793.0, 233165.0, 246108.0, 106781.0, 107226.0, 99620.0, 152381.0, 131506.0, 99733.0, 123359.0, 110807.0, 76331.0, 518895.0, 567952.0, 603664.0, 100603.0, 83615.0, 65341.0, 203538.0, 202794.0, 166206.0, 282294.0, 278605.0, 306706.0, 264677.0, 273173.0, 337454.0, 263413.0, 269335.0, 238478.0, 283626.0, 284279.0, 248686.0, 125669.0, 118439.0, 77935.0, 368052.0, 393892.0, 420597.0, 127571.0, 118643.0, 87909.0, 426557.0, 433918.0, 385953.0, 189718.0, 189750.0, 159400.0, 360617.0, 373747.0, 405112.0, 128585.0, 120653.0, 107187.0, 210364.0, 220418.0, 240045.0, 252943.0, 249492.0, 230501.0, 105996.0, 87883.0, 71625.0, 204538.0, 184442.0, 137207.0, 204956.0, 205964.0, 193618.0, 144718.0, 141496.0, 123245.0, 275210.0, 275082.0, 279964.0, 280244.0, 285939.0, 218704.0, 350045.0, 347268.0, 312444.0, 222111.0, 229999.0, 182906.0, 295947.0, 287196.0, 234200.0, 138054.0, 128800.0, 110377.0, 351286.0, 359705.0, 360909.0, 298984.0, 274622.0, 240366.0, 326533.0, 299677.0, 211270.0, 49182.0, 38974.0, 20343.0, 287288.0, 283830.0, 302899.0, 88408.0, 82421.0, 57396.0, 133268.0, 132852.0, 125933.0, 306776.0, 297426.0, 268565.0, 163677.0, 164658.0, 188413.0, 449805.0, 464019.0, 486684.0, 211679.0, 208529.0, 243696.0, 223314.0, 218337.0, 200370.0, 118172.0, 112766.0, 76147.0, 407780.0, 409621.0, 360440.0, 206468.0, 151818.0, 206368.0, 331150.0, 337643.0, 356106.0, 135866.0, 134431.0, 117702.0, 125451.0, 127950.0, 100460.0, 218081.0, 216815.0, 201325.0, 197142.0, 197782.0, 172668.0, 341854.0, 355434.0, 369687.0, 149983.0, 154865.0, 136488.0, 246706.0, 239236.0, 171082.0, 117806.0, 114210.0, 98392.0, 261535.0, 267132.0, 245801.0, 213111.0, 217268.0, 167838.0, 262140.0, 256363.0, 177016.0, 388009.0, 392956.0, 362103.0, 305683.0, 315353.0, 310778.0, 78147.0, 81185.0, 76466.0, 185086.0, 191568.0, 222892.0, 131638.0, 111736.0, 179406.0, 363707.0, 362110.0, 333326.0, 315141.0, 310434.0, 305630.0, 185232.0, 189310.0, 178266.0, 130944.0, 124559.0, 155344.0, 74383.0, 65159.0, 59772.0, 160052.0, 157398.0, 192492.0, 358792.0, 384759.0, 380742.0, 171144.0, 159157.0, 100877.0, 301013.0, 310779.0, 320939.0, 265649.0, 264592.0, 229172.0, 186815.0, 181261.0, 160578.0, 543336.0, 563376.0, 586904.0, 340092.0, 348853.0, 331675.0, 166267.0, 136130.0, 207313.0, 241526.0, 254172.0, 244910.0, 298187.0, 317044.0, 257988.0, 144694.0, 143108.0, 107338.0, 401848.0, 406915.0, 419174.0, 60540.0, 52061.0, 24376.0, 434905.0, 438262.0, 408345.0, 176549.0, 173593.0, 151189.0, 164635.0, 150285.0, 92178.0, 100309.0, 85348.0, 68729.0, 278720.0, 257862.0, 306170.0, 85472.0, 87016.0, 66526.0, 550148.0, 568324.0, 456167.0, 299930.0, 300750.0, 304166.0, 130534.0, 123343.0, 130709.0, 31614.0, 29341.0, 13783.0, 235713.0, 196533.0, 131515.0, 288460.0, 298189.0, 342414.0, 302076.0, 300585.0, 272763.0, 290233.0, 283757.0, 248973.0, 156663.0, 157044.0, 137939.0, 72178.0, 69055.0, 63543.0, 333081.0, 315161.0, 265972.0, 245424.0, 240142.0, 184965.0, 234338.0, 216858.0, 198822.0, 330830.0, 342145.0, 281386.0, 320627.0, 322980.0, 279234.0, 411617.0, 392800.0, 304262.0, 529278.0, 531337.0, 413098.0, 210170.0, 198077.0, 137855.0, 238543.0, 245251.0, 239074.0, 127327.0, 123328.0, 95347.0, 52618.0, 53558.0, 55249.0, 408411.0, 421520.0, 454319.0, 393205.0, 385992.0, 395364.0, 473339.0, 485760.0, 562344.0, 335501.0, 359377.0, 349659.0, 169235.0, 165161.0, 125520.0, 268592.0, 267473.0, 246060.0, 264328.0, 257751.0, 184101.0, 246310.0, 219803.0, 150002.0, 103453.0, 91886.0, 71109.0, 322568.0, 336502.0, 338314.0, 397896.0, 397704.0, 369677.0, 94644.0, 91000.0, 78227.0, 241107.0, 269359.0, 370093.0, 257823.0, 263105.0, 253143.0, 235314.0, 230778.0, 228456.0, 375958.0, 380830.0, 369037.0, 435032.0, 446921.0, 418836.0, 118360.0, 108834.0, 87607.0, 139338.0, 130530.0, 95497.0, 258518.0, 254789.0, 199712.0, 308007.0, 309709.0, 281443.0, 453916.0, 459488.0, 460403.0, 216739.0, 204080.0, 226907.0, 420940.0, 425265.0, 420954.0, 88601.0, 87935.0, 77415.0, 519564.0, 542431.0, 544553.0, 354774.0, 373791.0, 279649.0, 179065.0, 170176.0, 127848.0, 564164.0, 544525.0, 612098.0, 193615.0, 197879.0, 173083.0, 119045.0, 110217.0, 66657.0, 267039.0, 255518.0, 188716.0, 242310.0, 243728.0, 288319.0, 262185.0, 266424.0, 254270.0, 330712.0, 306364.0, 274136.0, 333868.0, 337598.0, 390258.0, 189129.0, 157258.0, 97075.0, 160703.0, 163026.0, 123995.0, 306836.0, 321848.0, 431794.0, 108183.0, 105212.0, 79009.0, 396835.0, 403734.0, 424402.0, 247962.0, 247913.0, 286245.0, 224886.0, 213990.0, 185085.0, 296835.0, 286062.0, 242244.0, 173292.0, 189753.0, 135639.0, 326082.0, 337177.0, 321223.0, 74748.0, 70303.0, 59047.0, 385239.0, 388490.0, 373160.0, 249687.0, 212989.0, 197402.0, 84470.0, 84170.0, 80379.0, 48772.0, 45593.0, 41431.0, 258893.0, 252644.0, 155912.0, 302281.0, 294628.0, 343002.0, 109882.0, 95300.0, 152773.0, 238267.0, 219932.0, 124438.0, 243551.0, 246963.0, 258163.0, 210583.0, 211232.0, 212119.0, 147687.0, 142171.0, 113115.0, 101519.0, 100562.0, 87280.0, 234559.0, 244756.0, 217373.0, 297845.0, 299567.0, 308252.0, 202089.0, 191535.0, 145335.0, 232354.0, 238377.0, 198722.0, 291395.0, 294247.0, 364252.0, 154235.0, 155900.0, 148035.0, 180358.0, 179590.0, 93392.0, 147708.0, 139480.0, 79802.0, 254502.0, 265595.0, 269060.0, 230246.0, 203803.0, 246295.0, 638110.0, 675928.0, 409069.0, 167400.0, 173654.0, 151667.0, 459564.0, 442043.0, 364255.0, 174986.0, 176863.0, 170733.0, 290357.0, 304456.0, 331953.0, 315305.0, 323295.0, 336770.0, 236814.0, 238074.0, 184994.0, 261340.0, 205665.0, 93537.0, 200696.0, 213748.0, 230264.0, 193569.0, 187949.0, 174786.0, 454552.0, 465859.0, 460159.0, 228586.0, 239434.0, 172337.0, 303561.0, 308880.0, 97015.0, 90340.0, 79116.0, 64669.0, 168589.0, 166070.0, 180238.0, 146486.0, 153896.0, 234027.0, 330630.0, 318740.0, 271704.0, 225939.0, 207975.0, 175184.0, 89068.0, 85391.0, 71514.0, 255990.0, 251743.0, 198603.0, 272065.0, 287350.0, 268662.0, 79119.0, 84610.0, 46412.0, 197561.0, 149261.0, 53015.0, 77739.0, 76305.0, 54427.0, 336672.0, 352352.0, 302665.0, 503402.0, 522065.0, 568029.0, 314269.0, 322822.0, 323702.0, 176711.0, 160312.0, 164341.0, 360931.0, 363551.0, 345913.0, 329116.0, 339138.0, 332407.0, 291992.0, 306034.0, 327872.0, 199752.0, 177414.0, 49104.0, 125467.0, 119076.0, 76240.0, 315318.0, 322222.0, 332019.0, 292541.0, 262209.0, 88199.0, 235741.0, 240180.0, 254829.0, 154909.0, 140380.0, 110845.0, 372750.0, 379024.0, 339701.0, 143264.0, 122086.0, 125442.0, 219408.0, 213524.0, 159519.0, 148907.0, 143136.0, 136354.0, 174284.0, 169718.0, 122306.0, 305750.0, 307250.0, 287445.0, 214882.0, 194684.0, 154634.0, 269337.0, 259134.0, 217370.0, 112153.0, 95508.0, 70922.0, 290018.0, 279007.0, 441960.0, 134651.0, 121906.0, 96182.0, 124112.0, 118589.0, 182210.0, 92051.0, 96673.0, 130680.0, 79324.0, 74219.0, 81415.0, 92229.0, 86965.0, 74367.0, 301423.0, 318951.0, 426784.0, 201782.0, 199755.0, 155125.0, 353402.0, 353923.0, 356283.0, 304209.0, 308020.0, 321270.0, 232084.0, 233753.0, 193993.0, 230007.0, 213274.0, 115610.0, 164443.0, 163225.0, 155031.0, 251952.0, 252129.0, 276541.0, 222652.0, 215298.0, 195735.0, 70621.0, 62407.0, 27729.0, 178791.0, 180369.0, 145783.0, 129732.0, 95499.0, 71942.0, 407506.0, 415224.0, 414841.0, 135594.0, 144180.0, 98222.0, 59261.0, 58639.0, 45201.0, 76171.0, 64197.0, 51546.0, 365870.0, 364968.0, 419917.0, 126364.0, 113204.0, 79780.0, 254195.0, 245683.0, 216698.0, 270758.0, 274419.0, 275716.0, 278246.0, 277418.0, 248655.0, 299842.0, 309187.0, 300933.0, 130652.0, 126032.0, 112734.0, 302533.0, 312041.0, 289416.0, 402368.0, 417479.0, 471210.0, 257685.0, 227755.0, 147657.0, 108336.0, 102655.0, 61530.0, 265217.0, 263984.0, 226680.0, 200916.0, 204715.0, 192245.0, 139419.0, 127343.0, 123981.0, 169172.0, 170024.0, 157596.0, 123275.0, 120656.0, 98720.0, 198036.0, 176142.0, 167625.0, 317742.0, 325440.0, 264797.0, 66621.0, 60011.0, 35374.0, 180401.0, 178102.0, 117423.0, 218519.0, 217127.0, 187931.0, 108784.0, 107983.0, 84075.0, 256567.0, 258977.0, 301013.0, 168611.0, 167736.0, 146516.0, 237181.0, 264303.0, 209540.0, 200339.0, 206965.0, 192646.0, 75775.0, 71087.0, 84087.0, 116873.0, 113776.0, 125376.0, 214972.0, 235614.0, 261576.0, 179304.0, 179616.0, 184737.0, 445356.0, 468151.0, 425403.0, 256867.0, 265180.0, 298641.0, 327835.0, 346769.0, 403353.0, 221385.0, 225403.0, 215982.0, 153974.0, 144120.0, 113024.0, 469953.0, 486384.0, 376088.0, 190789.0, 189440.0, 234383.0, 117694.0, 113039.0, 101865.0, 167034.0, 165157.0, 153676.0, 171072.0, 141719.0, 128491.0, 274815.0, 256291.0, 246261.0, 222923.0, 177406.0, 116964.0, 196373.0, 195191.0, 170675.0, 201614.0, 217715.0, 130309.0, 401710.0, 402897.0, 371406.0, 232637.0, 224000.0, 175608.0, 282680.0, 261865.0, 194668.0, 165535.0, 165559.0, 210692.0, 329392.0, 330333.0, 327931.0, 250202.0, 243595.0, 246892.0, 373301.0, 380960.0, 329577.0, 413323.0, 420615.0, 419949.0, 161641.0, 159001.0, 144138.0, 279636.0, 257678.0, 217245.0, 208012.0, 210919.0, 178017.0, 235816.0, 236413.0, 201934.0, 332596.0, 346599.0, 188629.0, 516505.0, 526963.0, 528199.0, 164601.0, 164557.0, 116845.0, 99898.0, 90343.0, 66651.0, 408050.0, 415828.0, 415948.0, 472643.0, 495895.0, 530975.0, 120529.0, 131416.0, 182389.0, 497425.0, 497223.0, 416208.0, 248543.0, 256300.0, 223655.0, 238306.0, 235332.0, 201562.0, 119386.0, 101040.0, 132097.0, 239195.0, 213347.0, 123323.0, 150398.0, 149351.0, 178863.0, 310086.0, 323362.0, 295017.0, 180151.0, 179058.0, 165145.0, 124191.0, 128150.0, 134430.0, 209661.0, 199694.0, 139297.0, 124370.0, 121260.0, 93028.0, 179741.0, 155558.0, 56412.0, 214889.0, 201089.0, 149340.0, 328207.0, 315934.0, 281948.0, 163950.0, 160532.0, 151193.0, 251833.0, 235758.0, 201139.0, 91541.0, 84026.0, 57459.0, 502482.0, 502180.0, 418100.0, 266586.0, 267543.0, 162287.0, 215744.0, 211496.0, 251881.0, 112858.0, 91515.0, 45039.0, 71112.0, 66478.0, 34430.0, 255493.0, 252952.0, 205170.0, 270033.0, 272298.0, 240082.0, 232579.0, 236454.0, 234665.0, 253129.0, 233019.0, 182600.0, 204650.0, 218160.0, 234852.0, 234579.0, 224441.0, 181119.0, 667410.0, 698349.0, 691944.0, 166426.0, 166654.0, 118437.0, 49485.0, 35674.0, 13436.0, 439515.0, 459429.0, 569459.0, 393505.0, 397091.0, 381862.0, 108857.0, 98764.0, 109009.0, 152471.0, 130503.0, 330724.0, 117478.0, 113316.0, 92003.0, 157861.0, 155159.0, 164964.0, 190513.0, 180102.0, 143285.0, 238931.0, 230582.0, 214486.0, 67790.0, 64118.0, 43617.0, 205762.0, 209103.0, 194530.0, 188979.0, 184372.0, 150018.0, 153734.0, 152770.0, 125969.0, 244454.0, 244022.0, 169139.0, 255434.0, 258965.0, 247306.0, 278220.0, 231079.0, 190909.0, 183688.0, 180589.0, 132473.0, 244293.0, 227139.0, 174921.0, 265386.0, 276410.0, 240185.0, 472228.0, 476377.0, 447009.0, 257276.0, 251769.0, 243030.0, 265242.0, 266931.0, 278774.0, 274384.0, 273405.0, 197740.0, 75454.0, 73678.0, 70573.0, 157862.0, 148010.0, 138455.0, 358664.0, 340164.0, 252164.0, 148539.0, 128996.0, 71030.0, 160815.0, 153841.0, 150577.0, 159223.0, 137162.0, 102436.0, 247069.0, 186890.0, 89559.0, 236697.0, 240493.0, 295724.0, 190520.0, 181813.0, 148575.0, 232231.0, 234187.0, 198512.0, 154505.0, 136047.0, 326077.0, 211958.0, 197249.0, 156055.0, 200101.0, 211027.0, 232384.0, 106000.0, 110472.0, 101056.0, 244746.0, 241365.0, 233542.0, 100423.0, 101935.0, 88959.0, 251018.0, 200528.0, 95186.0, 495806.0, 517190.0, 523696.0, 397909.0, 407449.0, 376097.0, 155596.0, 154768.0, 223184.0, 205739.0, 207854.0, 190487.0, 77410.0, 70636.0, 121086.0, 100887.0, 94061.0, 78944.0, 160288.0, 154679.0, 139145.0, 157817.0, 139452.0, 113834.0, 379652.0, 399454.0, 435226.0, 128039.0, 131648.0, 120584.0, 256237.0, 227319.0, 224292.0, 366188.0, 378649.0, 385409.0, 128909.0, 125807.0, 124535.0, 184882.0, 180806.0, 205647.0, 223318.0, 225920.0, 242438.0, 147050.0, 148461.0, 149889.0, 192056.0, 197408.0, 182501.0, 150709.0, 139453.0, 124182.0, 366552.0, 371904.0, 360107.0, 415431.0, 438659.0, 414051.0, 350322.0, 351218.0, 370669.0, 241635.0, 244323.0, 227489.0, 218993.0, 226507.0, 236949.0, 112196.0, 122038.0, 152672.0, 144952.0, 144268.0, 122576.0, 175343.0, 158593.0, 81266.0, 388419.0, 350573.0, 310603.0, 214236.0, 209241.0, 148695.0, 333410.0, 346572.0, 304325.0, 244227.0, 256057.0, 204465.0, 162951.0, 163700.0, 146186.0, 151175.0, 157960.0, 171299.0, 172695.0, 169483.0, 145319.0, 335588.0, 329690.0, 238542.0, 88342.0, 91653.0, 99371.0, 282316.0, 280344.0, 174320.0, 302060.0, 291833.0, 276430.0, 140074.0, 142472.0, 111254.0, 282380.0, 283705.0, 298755.0, 43359.0, 50547.0, 67013.0, 239077.0, 240028.0, 154707.0, 256143.0, 255013.0, 240421.0, 450308.0, 461560.0, 446406.0, 188372.0, 186304.0, 184523.0, 298339.0, 308057.0, 323345.0, 131628.0, 130148.0, 122540.0, 421746.0, 390156.0, 290529.0, 190465.0, 187104.0, 152588.0, 473328.0, 483967.0, 491286.0, 386380.0, 390862.0, 390608.0, 389655.0, 401165.0, 439600.0, 86903.0, 78522.0, 106794.0, 86967.0, 72795.0, 41355.0, 315498.0, 318894.0, 318633.0, 71312.0, 68594.0, 51987.0, 193359.0, 219200.0, 202174.0, 145345.0, 149495.0, 116754.0, 290889.0, 292657.0, 208378.0, 299681.0, 306950.0, 188629.0, 447989.0, 462047.0, 475743.0, 110548.0, 103534.0, 57386.0, 337803.0, 324722.0, 269546.0, 120937.0, 111437.0, 81907.0, 207631.0, 201363.0, 148735.0, 243654.0, 234848.0, 103283.0, 143415.0, 139232.0, 159173.0, 370697.0, 389615.0, 382729.0, 483492.0, 490224.0, 459813.0, 213985.0, 219710.0, 380390.0, 192580.0, 195083.0, 135849.0, 224401.0, 228059.0, 298258.0, 458206.0, 477650.0, 341807.0, 198233.0, 220226.0, 151757.0, 165710.0, 147504.0, 108120.0, 212024.0, 210705.0, 203004.0, 108500.0, 96827.0, 67189.0, 75935.0, 71389.0, 36682.0, 73749.0, 66366.0, 55571.0, 143640.0, 132763.0, 106273.0, 193893.0, 199405.0, 205296.0, 66463.0, 64310.0, 63110.0, 171466.0, 187856.0, 107908.0, 223091.0, 225058.0, 249095.0, 98997.0, 80970.0, 27234.0, 306865.0, 313282.0, 305539.0, 122011.0, 120521.0, 92161.0, 194583.0, 190839.0, 157374.0, 123044.0, 135218.0, 157542.0, 323969.0, 329471.0, 425886.0, 73848.0, 72108.0, 63885.0, 66599.0, 64235.0, 77512.0, 574857.0, 585358.0, 584508.0, 379382.0, 386498.0, 285539.0, 115273.0, 112466.0, 101032.0, 270517.0, 267371.0, 177170.0, 130991.0, 130497.0, 124879.0, 344540.0, 310114.0, 215954.0, 203031.0, 178939.0, 166916.0, 330283.0, 341377.0, 308906.0, 396727.0, 407995.0, 437775.0, 331869.0, 346025.0, 326396.0, 242669.0, 243087.0, 240225.0, 363655.0, 342527.0, 312306.0, 144572.0, 133893.0, 94868.0, 284899.0, 272836.0, 244305.0, 534260.0, 540574.0, 519945.0, 199262.0, 196611.0, 167391.0, 198490.0, 190955.0, 243466.0, 125887.0, 123075.0, 123906.0, 82794.0, 77909.0, 56604.0, 190987.0, 183509.0, 164922.0, 359712.0, 354000.0, 369380.0, 204162.0, 211118.0, 232683.0, 393380.0, 405009.0, 384191.0, 354260.0, 369214.0, 421358.0, 200611.0, 201022.0, 200492.0, 375695.0, 382041.0, 424491.0, 503821.0, 512217.0, 513559.0, 236125.0, 206084.0, 185493.0, 630278.0, 651092.0, 659639.0, 361955.0, 367818.0, 287719.0, 382402.0, 386732.0, 326498.0, 279293.0, 287283.0, 269446.0, 196362.0, 174135.0, 125103.0, 422101.0, 430562.0, 359860.0, 239379.0, 213645.0, 160211.0, 165726.0, 139002.0, 102226.0, 234818.0, 215028.0, 167745.0, 103216.0, 97637.0, 89817.0, 104307.0, 101620.0, 87750.0, 329973.0, 349397.0, 346957.0, 160090.0, 129445.0, 46353.0, 282195.0, 244641.0, 82830.0, 141453.0, 141253.0, 128896.0, 230632.0, 236699.0, 250249.0, 199804.0, 176249.0, 89239.0, 345023.0, 332004.0, 247942.0, 143872.0, 138225.0, 118796.0, 165372.0, 154693.0, 133794.0, 86893.0, 78666.0, 94300.0, 209824.0, 209167.0, 176118.0, 117981.0, 106292.0, 66847.0, 359102.0, 361217.0, 343373.0, 284763.0, 289843.0, 292325.0, 374307.0, 370946.0, 317084.0, 301266.0, 300933.0, 342396.0, 238547.0, 239848.0, 207728.0, 277846.0, 272301.0, 191425.0, 396165.0, 341298.0, 147815.0, 193599.0, 192404.0, 149487.0, 318724.0, 313188.0, 225834.0, 299614.0, 301094.0, 263988.0, 163906.0, 134866.0, 267791.0, 435749.0, 453131.0, 455719.0, 269826.0, 267299.0, 198856.0, 123452.0, 123020.0, 121291.0, 103910.0, 110182.0, 110965.0, 128425.0, 121224.0, 143172.0, 474379.0, 471662.0, 406214.0, 204410.0, 194205.0, 169446.0, 229311.0, 233002.0, 225995.0, 172311.0, 165789.0, 204611.0, 331119.0, 346517.0, 266903.0, 215836.0, 208474.0, 196148.0, 296564.0, 287665.0, 222010.0, 145002.0, 145617.0, 112669.0, 19151.0, 19071.0, 13451.0, 124508.0, 114014.0, 83967.0, 241939.0, 239309.0, 221460.0, 497384.0, 524188.0, 570770.0, 360406.0, 372696.0, 401447.0, 511142.0, 469967.0, 432769.0, 394030.0, 398488.0, 398247.0, 49613.0, 47096.0, 28540.0, 299688.0, 321862.0, 349373.0, 164748.0, 166709.0, 95206.0, 358843.0, 328363.0, 254714.0, 201400.0, 201801.0, 195726.0, 223859.0, 197465.0, 150923.0, 216923.0, 220989.0, 207570.0, 280303.0, 280703.0, 239290.0, 258703.0, 259245.0, 244287.0, 369253.0, 382182.0, 380262.0, 157345.0, 161010.0, 200322.0, 244910.0, 252965.0, 259888.0, 328599.0, 334641.0, 333450.0, 105759.0, 93205.0, 62625.0, 165826.0, 173473.0, 177228.0, 553096.0, 595698.0, 494593.0, 216165.0, 198997.0, 143228.0, 376996.0, 384768.0, 364611.0, 179611.0, 185389.0, 183690.0, 219553.0, 216918.0, 246499.0, 262962.0, 269662.0, 210814.0, 67506.0, 64996.0, 45818.0, 163889.0, 165649.0, 132748.0, 211849.0, 181758.0, 87070.0, 149476.0, 140794.0, 123614.0, 173288.0, 159906.0, 154299.0, 168887.0, 167162.0, 185316.0, 157388.0, 154119.0, 116300.0, 117349.0, 112320.0, 82962.0, 449874.0, 451895.0, 424046.0, 212332.0, 219316.0, 119244.0, 158864.0, 163102.0, 138380.0, 207780.0, 229731.0, 284660.0, 392663.0, 422687.0, 505040.0, 260988.0, 245687.0, 218290.0, 208041.0, 202761.0, 158055.0, 90900.0, 80146.0, 99532.0, 320109.0, 312534.0, 289432.0, 205020.0, 207668.0, 187657.0, 115140.0, 115439.0, 96275.0, 344754.0, 355547.0, 363704.0, 218902.0, 212165.0, 193185.0, 477588.0, 488987.0, 491843.0, 372791.0, 367671.0, 257480.0, 108596.0, 101579.0, 107569.0, 167790.0, 168311.0, 196346.0, 171791.0, 169037.0, 107316.0, 239817.0, 243676.0, 278600.0, 239909.0, 202745.0, 153770.0, 164959.0, 159068.0, 104875.0, 214249.0, 213107.0, 198044.0, 426518.0, 418773.0, 361249.0, 311360.0, 294636.0, 205835.0, 274921.0, 272944.0, 211445.0, 128920.0, 88991.0, 47798.0, 415148.0, 419512.0, 384292.0, 294632.0, 300843.0, 257241.0, 308845.0, 304519.0, 334269.0, 307505.0, 271916.0, 259157.0, 269356.0, 258502.0, 250871.0, 250757.0, 242478.0, 195817.0, 165140.0, 155079.0, 126604.0, 298419.0, 290155.0, 243656.0, 190878.0, 176902.0, 94219.0, 129418.0, 127471.0, 229947.0, 344933.0, 349331.0, 341745.0, 529869.0, 544511.0, 543646.0, 107690.0, 102629.0, 83658.0, 267516.0, 267208.0, 287124.0, 144371.0, 137176.0, 95226.0, 346678.0, 354448.0, 320259.0, 291758.0, 279016.0, 237164.0, 190790.0, 191126.0, 161705.0, 166422.0, 161278.0, 166876.0, 160680.0, 163971.0, 178314.0, 193141.0, 192030.0, 125365.0, 156781.0, 149534.0, 136579.0, 214592.0, 219394.0, 249344.0, 257713.0, 266324.0, 317895.0, 256780.0, 235449.0, 195920.0, 328298.0, 330650.0, 291757.0, 178741.0, 174610.0, 147306.0, 162825.0, 162193.0, 137374.0, 279759.0, 286482.0, 292325.0, 282738.0, 291507.0, 281467.0, 319250.0, 322119.0, 300082.0, 304698.0, 305917.0, 269733.0, 368288.0, 356085.0, 293840.0, 441725.0, 451795.0, 284002.0, 301912.0, 319139.0, 306503.0, 210173.0, 203043.0, 170366.0, 152565.0, 146832.0, 114787.0, 114160.0, 109318.0, 108697.0, 126170.0, 105810.0, 57945.0, 379470.0, 375179.0, 323091.0, 250313.0, 252931.0, 295369.0, 273113.0, 298094.0, 255969.0, 128395.0, 127507.0, 97864.0, 271567.0, 260251.0, 240009.0, 189151.0, 173651.0, 177124.0, 192230.0, 180280.0, 174988.0, 231982.0, 257996.0, 435498.0, 277695.0, 277477.0, 271729.0, 232106.0, 223623.0, 181772.0, 274314.0, 207677.0, 143350.0, 157844.0, 142546.0, 133359.0, 150148.0, 154753.0, 153529.0, 142145.0, 142110.0, 135726.0, 117541.0, 111899.0, 86549.0, 298785.0, 301647.0, 290535.0, 47424.0, 43501.0, 41577.0, 76487.0, 73547.0, 57123.0, 191399.0, 181791.0, 167985.0, 296535.0, 283716.0, 269088.0, 113359.0, 109051.0, 148493.0, 219992.0, 226851.0, 300792.0, 102195.0, 102121.0, 106308.0, 187692.0, 187098.0, 164544.0, 255568.0, 239820.0, 375239.0, 409359.0, 422050.0, 438636.0, 81834.0, 67595.0, 35714.0, 209926.0, 205978.0, 151593.0, 139341.0, 131820.0, 100229.0, 288824.0, 278531.0, 243823.0, 340547.0, 348873.0, 375269.0, 198356.0, 198341.0, 154055.0, 52057.0, 49312.0, 37781.0, 284154.0, 295957.0, 309266.0, 63200.0, 58640.0, 48333.0, 247020.0, 248460.0, 236327.0, 300676.0, 299469.0, 285850.0, 231454.0, 231303.0, 187194.0, 343413.0, 349004.0, 326563.0, 69622.0, 63762.0, 45091.0, 219340.0, 267601.0, 252407.0, 124323.0, 145434.0, 141816.0, 294376.0, 317191.0, 228516.0, 71132.0, 64234.0, 44413.0, 160916.0, 151962.0, 133977.0, 250536.0, 243453.0, 249719.0, 154194.0, 126270.0, 75161.0, 92299.0, 92731.0, 81289.0, 129432.0, 116420.0, 82213.0, 419577.0, 439571.0, 328405.0, 77392.0, 74422.0, 69709.0, 85917.0, 68219.0, 48922.0, 182991.0, 177950.0, 120242.0, 111193.0, 106578.0, 91339.0, 243897.0, 221808.0, 187463.0, 299854.0, 298661.0, 292397.0, 168140.0, 164052.0, 148149.0, 82294.0, 73478.0, 33196.0, 440425.0, 446740.0, 427180.0, 252694.0, 247931.0, 260313.0, 53683.0, 53806.0, 48307.0, 368000.0, 374908.0, 409548.0, 103948.0, 94409.0, 147472.0, 83110.0, 70758.0, 44490.0, 67500.0, 60086.0, 45263.0, 298366.0, 275759.0, 285852.0, 425523.0, 437573.0, 406147.0, 287182.0, 263725.0, 168412.0, 162744.0, 150483.0, 112237.0, 315629.0, 307794.0, 283016.0, 222511.0, 189332.0, 127383.0, 185295.0, 185357.0, 131939.0, 372822.0, 384091.0, 364129.0, 244182.0, 240746.0, 185074.0, 211247.0, 205881.0, 155066.0, 187644.0, 160640.0, 119606.0, 64228.0, 59951.0, 48200.0, 87051.0, 84022.0, 65704.0, 280540.0, 258459.0, 271305.0, 173177.0, 167528.0, 187229.0, 522665.0, 532270.0, 435241.0, 316834.0, 319143.0, 301066.0, 427432.0, 438547.0, 422642.0, 91537.0, 89786.0, 53764.0, 137252.0, 136103.0, 145805.0, 188686.0, 187255.0, 210515.0, 221604.0, 216008.0, 148138.0, 278926.0, 267039.0, 212655.0, 118874.0, 117432.0, 110350.0, 213047.0, 216198.0, 209495.0, 304056.0, 304957.0, 335991.0, 200376.0, 192997.0, 165346.0, 143988.0, 133379.0, 128505.0, 216401.0, 194736.0, 162410.0, 239250.0, 224573.0, 271991.0, 141302.0, 119974.0, 71047.0, 128012.0, 122694.0, 95914.0, 368237.0, 374026.0, 354750.0, 321552.0, 318971.0, 317235.0, 455651.0, 446806.0, 450215.0, 263036.0, 263969.0, 254702.0, 289940.0, 267905.0, 235251.0, 423802.0, 420560.0, 355635.0, 202674.0, 195789.0, 171666.0, 177639.0, 174981.0, 173671.0, 406494.0, 419496.0, 420906.0, 163395.0, 155207.0, 117665.0, 144861.0, 143279.0, 123822.0, 491452.0, 500625.0, 497949.0, 99784.0, 93507.0, 67792.0, 188713.0, 184467.0, 142397.0, 368773.0, 368904.0, 348109.0, 69678.0, 63656.0, 45925.0, 169134.0, 176426.0, 146316.0, 289404.0, 277668.0, 212173.0, 110466.0, 101561.0, 58838.0, 339488.0, 318824.0, 327529.0, 209132.0, 206176.0, 173072.0, 130790.0, 124653.0, 102351.0, 250470.0, 252815.0, 220334.0, 398256.0, 384467.0, 362328.0, 342203.0, 325940.0, 264491.0, 341299.0, 326085.0, 256076.0, 195218.0, 194537.0, 139698.0, 184313.0, 184678.0, 125144.0, 218474.0, 227557.0, 234933.0, 388913.0, 397219.0, 416022.0, 262121.0, 243502.0, 207417.0, 153298.0, 147144.0, 189343.0, 231519.0, 222800.0, 188290.0, 132233.0, 124998.0, 86693.0, 173898.0, 144220.0, 71775.0, 156721.0, 150773.0, 141791.0, 177767.0, 158419.0, 133731.0, 119371.0, 94439.0, 56654.0, 132072.0, 136244.0, 53680.0, 258660.0, 252118.0, 227126.0, 250248.0, 248327.0, 173033.0, 468301.0, 487555.0, 537224.0, 81438.0, 71721.0, 122799.0, 251014.0, 240659.0, 202761.0, 294157.0, 293145.0, 267787.0, 186776.0, 170482.0, 155207.0, 294288.0, 271663.0, 309883.0, 146650.0, 142616.0, 141277.0, 439803.0, 451717.0, 435309.0, 191294.0, 185308.0, 361328.0, 415509.0, 435740.0, 324351.0, 381554.0, 385118.0, 409311.0, 141628.0, 136373.0, 133986.0, 541815.0, 544309.0, 518946.0, 338597.0, 344022.0, 293212.0, 310709.0, 318750.0, 316542.0, 287242.0, 302143.0, 351758.0, 229415.0, 232666.0, 224223.0, 346153.0, 304865.0, 152576.0, 407031.0, 398220.0, 323972.0, 545031.0, 563033.0, 610442.0, 228205.0, 200873.0, 103194.0, 196455.0, 154852.0, 55107.0, 216516.0, 197580.0, 129492.0, 318611.0, 324989.0, 315513.0, 254273.0, 239977.0, 114339.0, 89132.0, 77932.0, 103506.0, 476705.0, 477533.0, 515206.0, 346535.0, 337349.0, 327037.0, 99884.0, 99965.0, 92282.0, 270431.0, 256645.0, 227906.0, 364317.0, 364533.0, 392749.0, 362478.0, 383858.0, 377342.0, 207715.0, 186032.0, 148544.0, 394028.0, 418302.0, 460808.0, 127520.0, 121320.0, 109014.0, 125982.0, 121374.0, 101933.0, 294808.0, 300676.0, 280684.0, 288782.0, 285220.0, 173220.0, 245147.0, 232975.0, 196547.0, 249313.0, 255268.0, 204333.0, 161429.0, 154751.0, 133896.0, 231395.0, 218229.0, 162845.0, 140255.0, 128893.0, 92640.0, 291057.0, 305952.0, 317136.0, 194331.0, 167388.0, 123173.0, 140441.0, 127269.0, 93761.0, 115987.0, 90804.0, 70692.0, 145244.0, 115448.0, 212832.0, 472682.0, 456053.0, 475281.0, 306220.0, 296888.0, 290630.0, 383891.0, 374641.0, 278003.0, 286767.0, 282187.0, 211939.0, 160663.0, 160461.0, 130152.0, 90614.0, 72196.0, 42501.0, 214079.0, 222911.0, 221069.0, 131100.0, 117759.0, 102171.0, 577296.0, 599590.0, 633116.0, 154152.0, 152333.0, 148804.0, 118695.0, 114803.0, 82233.0, 87302.0, 74350.0, 42518.0, 426235.0, 443083.0, 476197.0, 225480.0, 235055.0, 229127.0, 266635.0, 228846.0, 181972.0, 145612.0, 150390.0, 149572.0, 216591.0, 223012.0, 193520.0, 480481.0, 502151.0, 540713.0, 293912.0, 254046.0, 180344.0, 172269.0, 176031.0, 167989.0, 327221.0, 355485.0, 329017.0, 279036.0, 259624.0, 242730.0, 174050.0, 154774.0, 114793.0, 236863.0, 214627.0, 155191.0, 260623.0, 255472.0, 186912.0, 204933.0, 202685.0, 207021.0, 247563.0, 236799.0, 191304.0, 136007.0, 110727.0, 185513.0, 136349.0, 120100.0, 117413.0, 175076.0, 176225.0, 206460.0, 262407.0, 254179.0, 137220.0, 201449.0, 182561.0, 105174.0, 89452.0, 81724.0, 130506.0, 90342.0, 90671.0, 82718.0, 78561.0, 65749.0, 60292.0, 258333.0, 223422.0, 145874.0, 240289.0, 246644.0, 253717.0, 306787.0, 295502.0, 241586.0, 70858.0, 70483.0, 57471.0, 261536.0, 262267.0, 187096.0, 388983.0, 392807.0, 385065.0, 78351.0, 80387.0, 52726.0, 328410.0, 318618.0, 225422.0, 180838.0, 173286.0, 118722.0, 69072.0, 65911.0, 55900.0, 127316.0, 131709.0, 118463.0, 87397.0, 80417.0, 105078.0, 158169.0, 166355.0, 112788.0, 262652.0, 271483.0, 216011.0, 203447.0, 209007.0, 194875.0, 244665.0, 232173.0, 178639.0, 292654.0, 295783.0, 265712.0, 312536.0, 322872.0, 297123.0, 245724.0, 237280.0, 214699.0, 60777.0, 49232.0, 35829.0, 156881.0, 136694.0, 71298.0, 408362.0, 409710.0, 400842.0, 187497.0, 180686.0, 121499.0, 360198.0, 352970.0, 351247.0, 273323.0, 276865.0, 237888.0, 166052.0, 152596.0, 83460.0, 118160.0, 111774.0, 132509.0, 144581.0, 141671.0, 123678.0, 262305.0, 269099.0, 271363.0, 280419.0, 262477.0, 206290.0, 261578.0, 240501.0, 178826.0, 215037.0, 216101.0, 214210.0, 288032.0, 296938.0, 278402.0, 184485.0, 173175.0, 141587.0, 272375.0, 242213.0, 121576.0, 193160.0, 186145.0, 145107.0, 404468.0, 404991.0, 259120.0, 136947.0, 132200.0, 120173.0, 102633.0, 102904.0, 126742.0, 254569.0, 251657.0, 250494.0, 112926.0, 100552.0, 100661.0, 379696.0, 364926.0, 317723.0, 363218.0, 320275.0, 275629.0, 198853.0, 195184.0, 161166.0, 401214.0, 397569.0, 502298.0, 158586.0, 138442.0, 115168.0, 265610.0, 236910.0, 160933.0, 131393.0, 125098.0, 128375.0, 104156.0, 99704.0, 74408.0, 75653.0, 72332.0, 41483.0, 245904.0, 255037.0, 244974.0, 535860.0, 534277.0, 472983.0, 194716.0, 169813.0, 210367.0, 266139.0, 269519.0, 231090.0, 100690.0, 102331.0, 89565.0, 134614.0, 124070.0, 95438.0, 113966.0, 113932.0, 108673.0, 284979.0, 282171.0, 357927.0, 234454.0, 244372.0, 181823.0, 350008.0, 372896.0, 417150.0, 194041.0, 180662.0, 130911.0, 357502.0, 373227.0, 404467.0, 222848.0, 222831.0, 153060.0, 203707.0, 224091.0, 243891.0, 197641.0, 196891.0, 186069.0, 93938.0, 88237.0, 70416.0, 139261.0, 127957.0, 95388.0, 59267.0, 53918.0, 35418.0, 28917.0, 26821.0, 11969.0, 67962.0, 65539.0, 63608.0, 164610.0, 170287.0, 170848.0, 132834.0, 122609.0, 90568.0, 92685.0, 89567.0, 85511.0, 258206.0, 240572.0, 191268.0, 401609.0, 411567.0, 396651.0, 208649.0, 204083.0, 140435.0, 370528.0, 383722.0, 387550.0, 326301.0, 336978.0, 338234.0, 107359.0, 105237.0, 80191.0, 69321.0, 64992.0, 51959.0, 54151.0, 51681.0, 54738.0, 270377.0, 255805.0, 165013.0, 385778.0, 393339.0, 323407.0, 159654.0, 164599.0, 147752.0, 81449.0, 76536.0, 67108.0, 164643.0, 167675.0, 119457.0, 195239.0, 191931.0, 207568.0, 244606.0, 246192.0, 239251.0, 266090.0, 261885.0, 195788.0, 73352.0, 53998.0, 34998.0, 485521.0, 484124.0, 514616.0, 127379.0, 110821.0, 86999.0, 107560.0, 87287.0, 63221.0, 151771.0, 162727.0, 107069.0, 82785.0, 80041.0, 68913.0, 229257.0, 228298.0, 226818.0, 201652.0, 212274.0, 209770.0, 284759.0, 269675.0, 209577.0, 159586.0, 155297.0, 127037.0, 158742.0, 159427.0, 121673.0, 310147.0, 313387.0, 297563.0, 338935.0, 303753.0, 197613.0, 226887.0, 219847.0, 129813.0, 88247.0, 76233.0, 41718.0, 399515.0, 391471.0, 316743.0, 279970.0, 292937.0, 198731.0, 280060.0, 285025.0, 316718.0, 157570.0, 159972.0, 151348.0, 190100.0, 174050.0, 161206.0, 62640.0, 59437.0, 39279.0, 260218.0, 257388.0, 266296.0, 387529.0, 392455.0, 393260.0, 80120.0, 89899.0, 53998.0, 235526.0, 218432.0, 223972.0, 207430.0, 195579.0, 196858.0, 166141.0, 144164.0, 107456.0, 295719.0, 269920.0, 131994.0, 331968.0, 317312.0, 246385.0, 146724.0, 142426.0, 80391.0, 465761.0, 460426.0, 410366.0, 277444.0, 282997.0, 215769.0, 297160.0, 295896.0, 237625.0, 215528.0, 213456.0, 124061.0, 225469.0, 217094.0, 194854.0, 199834.0, 192948.0, 168703.0, 189270.0, 189051.0, 159810.0, 108062.0, 106968.0, 105233.0, 272572.0, 192764.0, 375362.0, 398808.0, 421156.0, 456868.0, 416658.0, 411368.0, 415086.0, 261661.0, 265929.0, 242959.0, 373465.0, 388158.0, 419481.0, 384523.0, 398434.0, 415529.0, 254544.0, 246633.0, 201006.0, 539368.0, 555908.0, 577930.0, 298182.0, 301375.0, 274112.0, 287668.0, 291250.0, 271595.0, 205454.0, 202688.0, 150547.0, 138463.0, 138403.0, 126063.0, 208712.0, 212460.0, 170617.0, 237947.0, 232621.0, 211141.0, 231361.0, 213131.0, 192012.0, 167489.0, 168470.0, 94198.0, 146628.0, 146809.0, 118754.0, 152793.0, 155066.0, 143837.0, 253273.0, 260676.0, 269310.0, 133493.0, 125821.0, 124734.0, 268155.0, 237569.0, 87752.0, 321826.0, 337460.0, 285310.0, 123647.0, 121551.0, 122418.0, 193449.0, 168054.0, 136559.0, 226339.0, 185691.0, 132655.0, 156572.0, 142786.0, 204029.0, 162111.0, 141760.0, 78664.0, 274504.0, 274528.0, 326103.0, 209156.0, 198926.0, 182249.0, 219268.0, 206857.0, 144242.0, 377065.0, 379343.0, 330415.0, 98366.0, 96659.0, 76205.0, 147791.0, 145962.0, 133943.0, 140394.0, 142890.0, 149888.0, 246398.0, 249254.0, 199906.0, 199592.0, 192517.0, 159331.0, 170745.0, 173066.0, 157040.0, 187578.0, 198610.0, 222600.0, 238539.0, 234608.0, 241008.0, 135324.0, 129463.0, 113920.0, 308088.0, 306156.0, 305286.0, 114447.0, 100581.0, 55566.0, 211589.0, 209818.0, 183175.0, 260616.0, 256587.0, 194941.0, 482518.0, 498412.0, 526471.0, 181253.0, 181731.0, 213281.0, 439792.0, 453943.0, 496624.0, 49689.0, 50798.0, 39154.0, 348341.0, 360004.0, 256438.0, 113641.0, 118754.0, 129735.0, 294305.0, 307326.0, 238683.0, 152488.0, 139135.0, 99077.0, 241905.0, 225477.0, 144449.0, 364640.0, 380931.0, 387284.0, 390432.0, 378772.0, 253832.0, 215345.0, 197531.0, 157613.0, 138695.0, 141989.0, 155633.0, 218095.0, 218613.0, 195942.0, 288830.0, 284571.0, 218300.0, 403893.0, 393788.0, 336626.0, 240486.0, 235663.0, 221663.0, 160785.0, 153558.0, 123993.0, 450266.0, 455209.0, 422900.0, 185897.0, 174851.0, 202427.0, 206856.0, 177370.0, 143348.0, 215639.0, 207266.0, 200270.0, 269158.0, 268742.0, 282209.0, 227582.0, 229801.0, 240677.0, 308286.0, 292856.0, 193227.0, 171844.0, 167173.0, 138526.0, 148582.0, 128795.0, 92406.0, 157751.0, 154797.0, 118285.0, 255169.0, 253632.0, 231990.0, 181134.0, 179799.0, 169010.0, 571227.0, 580030.0, 555144.0, 161033.0, 147646.0, 99123.0, 71883.0, 73975.0, 66312.0, 335666.0, 344654.0, 389268.0, 234309.0, 232790.0, 206701.0, 152039.0, 144750.0, 97121.0, 251178.0, 228682.0, 175520.0, 206628.0, 203109.0, 188440.0, 80909.0, 71430.0, 56999.0, 216914.0, 216456.0, 209771.0, 187338.0, 188878.0, 153357.0, 246179.0, 240916.0, 239209.0, 194985.0, 180627.0, 105902.0, 175435.0, 183324.0, 108265.0, 146236.0, 145847.0, 145430.0, 142122.0, 141073.0, 127205.0, 229449.0, 224275.0, 198138.0, 163741.0, 152417.0, 128750.0, 108810.0, 102496.0, 78030.0, 200147.0, 212170.0, 205034.0, 220124.0, 215521.0, 185405.0, 88199.0, 80244.0, 60825.0, 285433.0, 290089.0, 283512.0, 49713.0, 50897.0, 38553.0, 346831.0, 339587.0, 290227.0, 175250.0, 159854.0, 125057.0, 190015.0, 196121.0, 193992.0, 647125.0, 661954.0, 693613.0, 172771.0, 159902.0, 105194.0, 238530.0, 225955.0, 207830.0, 165312.0, 170767.0, 164160.0, 275278.0, 268126.0, 218735.0, 181928.0, 168684.0, 95334.0, 240117.0, 240425.0, 225997.0, 249467.0, 251791.0, 255010.0, 336337.0, 323757.0, 282301.0, 399371.0, 410335.0, 400577.0, 250737.0, 220895.0, 152453.0, 143789.0, 147348.0, 78789.0, 135650.0, 134448.0, 99598.0, 390835.0, 380324.0, 312115.0, 80189.0, 72440.0, 43285.0, 187576.0, 178335.0, 161829.0, 84181.0, 87903.0, 72992.0, 200013.0, 161701.0, 199423.0, 132137.0, 118379.0, 72555.0, 317734.0, 324056.0, 325812.0, 98188.0, 83797.0, 95222.0, 77977.0, 71285.0, 38269.0, 348404.0, 353201.0, 357810.0, 342123.0, 358496.0, 325416.0, 155439.0, 150115.0, 109087.0, 236827.0, 243950.0, 209285.0, 287934.0, 282871.0, 233669.0, 255035.0, 236839.0, 212300.0, 381857.0, 383252.0, 357598.0, 297917.0, 295832.0, 234385.0, 234969.0, 237083.0, 232457.0, 251448.0, 247610.0, 205757.0, 256791.0, 283055.0, 392241.0, 350791.0, 352201.0, 325667.0, 327649.0, 326859.0, 267987.0, 208503.0, 216743.0, 165556.0, 164512.0, 167224.0, 99607.0, 404424.0, 412759.0, 517488.0, 169679.0, 146445.0, 190540.0, 280203.0, 261312.0, 216521.0, 250817.0, 264994.0, 312079.0, 74191.0, 71510.0, 65712.0, 190946.0, 196593.0, 173995.0, 186087.0, 184308.0, 137033.0, 391194.0, 400122.0, 460966.0, 107838.0, 105782.0, 82730.0, 198786.0, 192069.0, 160834.0, 139304.0, 143047.0, 115587.0, 360614.0, 376551.0, 381954.0, 143192.0, 142809.0, 139687.0, 106795.0, 104306.0, 90911.0, 106050.0, 86325.0, 45179.0, 204752.0, 215665.0, 66511.0, 152792.0, 153117.0, 159748.0, 219163.0, 221006.0, 233975.0, 412200.0, 404900.0, 314895.0, 360644.0, 363991.0, 313166.0, 199751.0, 195164.0, 161603.0, 143124.0, 138373.0, 121389.0, 231937.0, 241602.0, 257895.0, 396220.0, 406415.0, 425048.0, 175162.0, 171221.0, 76391.0, 146696.0, 178436.0, 73955.0, 238773.0, 242943.0, 258301.0, 197494.0, 187960.0, 180156.0, 316502.0, 304887.0, 259532.0, 314140.0, 266449.0, 129459.0, 150094.0, 153239.0, 220365.0, 109569.0, 106072.0, 112783.0, 344609.0, 334176.0, 185239.0, 92374.0, 94557.0, 72243.0, 125335.0, 127154.0, 95793.0, 181329.0, 184905.0, 182270.0, 71947.0, 73846.0, 99142.0, 209923.0, 199447.0, 248686.0, 45968.0, 37443.0, 15468.0, 120413.0, 118135.0, 116869.0, 373523.0, 366866.0, 349937.0, 169425.0, 164544.0, 105602.0, 159396.0, 139857.0, 101910.0, 169617.0, 168224.0, 113168.0, 169702.0, 172642.0, 163270.0, 159738.0, 147607.0, 86523.0, 272366.0, 292740.0, 253281.0, 210715.0, 219558.0, 212961.0, 41754.0, 27629.0, 21061.0, 189862.0, 180946.0, 179341.0, 84805.0, 79387.0, 73680.0, 89423.0, 86884.0, 80525.0, 130556.0, 131149.0, 131258.0, 311293.0, 311231.0, 252922.0, 214330.0, 208339.0, 171073.0, 183796.0, 180286.0, 139974.0, 318460.0, 320335.0, 303555.0, 117490.0, 121007.0, 96962.0, 526603.0, 540412.0, 522623.0, 280244.0, 286506.0, 274120.0, 260299.0, 261123.0, 291616.0, 84450.0, 77811.0, 123706.0, 232816.0, 217409.0, 148619.0, 364281.0, 370700.0, 364751.0, 60153.0, 58972.0, 57025.0, 112805.0, 99127.0, 122132.0, 107224.0, 104044.0, 78329.0, 266202.0, 205363.0, 112040.0, 157494.0, 146581.0, 122059.0, 152125.0, 140347.0, 71146.0, 505461.0, 507167.0, 393920.0, 165321.0, 154178.0, 187568.0, 150464.0, 127460.0, 92623.0, 41919.0, 30458.0, 13043.0, 84037.0, 88783.0, 96074.0, 151287.0, 149416.0, 141649.0, 155615.0, 149270.0, 110253.0, 295651.0, 288242.0, 214655.0, 157376.0, 145436.0, 86017.0, 184737.0, 174282.0, 125594.0, 533748.0, 528263.0, 375117.0, 68578.0, 66806.0, 51847.0, 215958.0, 184378.0, 118185.0, 178019.0, 173842.0, 153466.0, 613269.0, 626588.0, 612084.0, 267045.0, 247799.0, 236806.0, 230433.0, 209217.0, 244112.0, 161311.0, 157715.0, 145200.0, 176237.0, 159549.0, 325146.0, 319854.0, 321276.0, 325235.0, 63261.0, 59776.0, 49666.0, 108171.0, 94449.0, 102462.0, 157758.0, 159800.0, 160661.0, 95957.0, 91074.0, 69723.0, 263809.0, 248084.0, 171654.0, 200472.0, 199803.0, 192528.0, 58457.0, 60561.0, 57485.0, 180208.0, 181778.0, 180878.0, 119283.0, 103174.0, 76802.0, 275708.0, 261458.0, 170771.0, 435850.0, 455059.0, 479284.0, 120773.0, 105009.0, 81912.0, 310411.0, 325785.0, 354338.0, 171206.0, 132969.0, 88445.0, 325353.0, 335700.0, 252874.0, 541996.0, 561342.0, 590425.0, 241923.0, 247573.0, 221363.0, 263962.0, 258411.0, 205864.0, 234789.0, 240314.0, 228357.0, 205801.0, 208032.0, 225746.0, 292438.0, 292194.0, 243625.0, 275879.0, 286153.0, 208337.0, 162116.0, 141841.0, 183042.0, 154512.0, 156094.0, 266357.0, 31172.0, 32277.0, 23878.0, 66691.0, 63715.0, 52806.0, 165906.0, 163914.0, 145717.0, 308078.0, 294753.0, 285685.0, 161580.0, 117495.0, 212915.0, 197624.0, 190258.0, 165751.0, 179599.0, 133998.0, 46501.0, 371141.0, 392135.0, 412723.0, 147626.0, 128662.0, 134276.0, 208816.0, 170522.0, 471570.0, 185597.0, 153459.0, 136328.0, 261567.0, 267312.0, 223337.0, 132409.0, 128288.0, 162474.0, 366808.0, 382120.0, 278512.0, 502986.0, 500753.0, 485078.0, 289108.0, 312463.0, 311773.0, 158225.0, 131198.0, 118022.0, 280294.0, 288215.0, 282179.0, 334118.0, 326794.0, 300469.0, 243885.0, 253422.0, 199024.0, 71880.0, 72221.0, 51357.0, 168292.0, 160489.0, 104500.0, 330400.0, 289710.0, 195708.0, 237237.0, 243858.0, 179162.0, 235290.0, 231908.0, 193817.0, 335532.0, 334512.0, 482525.0, 357207.0, 317929.0, 232777.0, 253511.0, 251510.0, 238419.0, 248077.0, 245201.0, 196351.0, 65973.0, 64656.0, 46641.0, 206369.0, 215419.0, 239960.0, 96600.0, 91662.0, 70963.0, 234811.0, 238830.0, 275188.0, 348856.0, 366258.0, 398923.0, 232757.0, 232044.0, 309577.0, 145668.0, 146818.0, 126314.0, 195966.0, 190894.0, 179289.0, 534106.0, 529492.0, 437311.0, 258874.0, 252188.0, 177527.0, 147841.0, 147477.0, 121100.0, 82256.0, 52367.0, 132711.0, 174533.0, 126146.0, 182298.0, 357623.0, 364323.0, 348043.0, 251280.0, 238618.0, 205152.0, 290974.0, 302998.0, 374293.0, 253986.0, 255800.0, 266116.0, 222423.0, 216221.0, 227900.0, 188772.0, 193222.0, 184301.0, 384988.0, 379568.0, 233601.0, 252526.0, 251577.0, 145225.0, 173738.0, 177730.0, 161620.0, 133334.0, 135960.0, 128437.0, 257941.0, 267008.0, 215362.0, 386080.0, 394885.0, 363015.0, 397538.0, 400001.0, 432418.0, 383799.0, 388868.0, 357639.0, 345567.0, 314164.0, 262052.0, 280288.0, 289686.0, 380483.0, 149106.0, 147924.0, 124139.0, 282189.0, 286224.0, 239157.0, 354176.0, 363845.0, 374372.0, 387052.0, 390367.0, 417355.0, 108504.0, 102056.0, 67288.0, 170012.0, 170596.0, 182527.0, 372774.0, 384432.0, 394602.0, 275028.0, 269586.0, 234490.0, 164790.0, 153180.0, 150215.0, 172279.0, 161698.0, 115255.0, 301379.0, 310429.0, 276058.0, 335924.0, 336161.0, 245073.0, 273840.0, 260327.0, 183793.0, 53951.0, 52994.0, 45965.0, 419924.0, 434935.0, 419364.0, 331451.0, 327144.0, 323207.0, 249080.0, 237748.0, 105495.0, 439776.0, 433930.0, 344068.0, 133324.0, 128445.0, 103141.0, 197603.0, 219996.0, 156144.0, 422682.0, 426477.0, 418623.0, 184028.0, 172988.0, 135938.0, 226049.0, 224043.0, 142900.0, 291682.0, 258184.0, 235234.0, 107551.0, 94613.0, 63542.0, 173513.0, 128063.0, 63096.0, 216321.0, 209308.0, 166786.0, 126089.0, 118454.0, 150460.0, 144433.0, 135011.0, 50194.0, 141757.0, 124790.0, 65044.0, 92531.0, 90085.0, 59967.0, 194123.0, 194814.0, 194797.0, 305481.0, 292443.0, 209096.0, 155982.0, 153614.0, 132426.0, 278146.0, 274609.0, 235006.0, 142868.0, 141971.0, 98955.0, 288299.0, 279312.0, 264845.0, 253093.0, 246357.0, 199582.0, 218139.0, 212515.0, 186373.0, 225385.0, 214679.0, 185856.0, 85018.0, 78578.0, 103720.0, 161558.0, 138615.0, 167017.0, 285601.0, 294741.0, 313350.0, 269328.0, 271387.0, 235528.0, 326416.0, 328621.0, 338580.0, 294045.0, 286401.0, 252520.0, 231557.0, 230692.0, 207236.0, 449401.0, 463672.0, 471464.0, 257289.0, 296464.0, 183848.0, 315253.0, 337526.0, 153310.0, 64081.0, 64631.0, 61712.0, 166366.0, 169941.0, 178848.0]\n"
     ]
    }
   ],
   "source": [
    "# test torch.mean(self.X_train, dim=(0, 2, 3)) and torch.std(self.X_train, dim=(0, 2, 3)) self.X_train is a n*3*64*64 tensor\n",
    "# n is the number of pictures\n",
    "current_dir = os.getcwd()\n",
    "X_val = []\n",
    "train_list = os.listdir(current_dir + '/data/train')\n",
    "val_list = os.listdir(current_dir + '/data/val')\n",
    "for i in val_list:\n",
    "    pic_list = os.listdir(current_dir + '/data/val/' + i)\n",
    "    for j in pic_list:\n",
    "        img = Image.open(current_dir + '/data/val/' + i + '/' + j)\n",
    "        img = np.array(img)\n",
    "        img = torch.tensor(img)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        img = img.unsqueeze(0).float()\n",
    "        X_val.append(img)\n",
    "X_val = torch.cat(X_val)\n",
    "print(X_val.shape)\n",
    "\n",
    "# init channel sum and pixel sum\n",
    "channel_sum = [0, 0, 0]\n",
    "channel_squared_sum = [0, 0, 0]\n",
    "pixel_sum = 64*64*len(X_val)\n",
    "for i in X_val:\n",
    "    channel_sum += torch.sum(i, dim=(1, 2)).tolist()\n",
    "    channel_squared_sum += torch.sum(i**2, dim=(1, 2)).tolist()\n",
    "channel_mean = [i / pixel_sum for i in channel_sum]\n",
    "channel_std = [np.sqrt(i / pixel_sum - j**2) for i, j in zip(channel_squared_sum, channel_mean)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
